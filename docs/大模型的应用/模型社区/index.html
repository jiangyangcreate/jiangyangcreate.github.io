<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-docs docs-version-current docs-doc-page docs-doc-id-大模型的应用/模型社区" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">模型社区 | jiangmiemie</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jiangmiemie.com/pages/case/jiangmiemie.webp"><meta data-rh="true" name="twitter:image" content="https://jiangmiemie.com/pages/case/jiangmiemie.webp"><meta data-rh="true" property="og:url" content="https://jiangmiemie.com/docs/大模型的应用/模型社区/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="docusaurus,blog, python, 开源"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-docs-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-docs-current"><meta data-rh="true" property="og:title" content="模型社区 | jiangmiemie"><meta data-rh="true" name="description" content="本部分内容告诉你如何选择或者微调量化出功能及大小合适的模型并部署。"><meta data-rh="true" property="og:description" content="本部分内容告诉你如何选择或者微调量化出功能及大小合适的模型并部署。"><link data-rh="true" rel="icon" href="/favicon.ico"><link data-rh="true" rel="canonical" href="https://jiangmiemie.com/docs/大模型的应用/模型社区/"><link data-rh="true" rel="alternate" href="https://jiangmiemie.com/docs/大模型的应用/模型社区/" hreflang="en"><link data-rh="true" rel="alternate" href="https://jiangmiemie.com/docs/大模型的应用/模型社区/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://B8DUWB4CMX-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"大模型的应用","item":"https://jiangmiemie.com/docs/大模型的应用/"},{"@type":"ListItem","position":2,"name":"模型社区","item":"https://jiangmiemie.com/docs/大模型的应用/模型社区"}]}</script><link rel="search" type="application/opensearchdescription+xml" title="jiangmiemie" href="/opensearch.xml">
<link rel="icon" href="/img/logo-512.svg">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37, 194, 160)">


<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="jiangmiemie RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="jiangmiemie Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="jiangmiemie JSON Feed">


<link rel="stylesheet" href="/katex/katex.min.css"><link rel="stylesheet" href="/assets/css/styles.57232b87.css">
<script src="/assets/js/runtime~main.4b3656d6.js" defer="defer"></script>
<script src="/assets/js/main.381a0d14.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/">开发</a><a class="navbar__item navbar__link" href="/read/">书架</a><a class="navbar__item navbar__link" href="/blog/archive/">博文</a><a class="navbar__item navbar__link" href="/case/">个案</a><a class="navbar__item navbar__link" href="/gallery/">相簿</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/"><span title="编程外的基础" class="categoryLinkLabel_W154">编程外的基础</span></a><button aria-label="Expand sidebar category &#x27;编程外的基础&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/数据分析/"><span title="数据分析" class="categoryLinkLabel_W154">数据分析</span></a><button aria-label="Expand sidebar category &#x27;数据分析&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/数据库与消息队列/"><span title="数据库与消息队列" class="categoryLinkLabel_W154">数据库与消息队列</span></a><button aria-label="Expand sidebar category &#x27;数据库与消息队列&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/编程语言/"><span title="编程语言" class="categoryLinkLabel_W154">编程语言</span></a><button aria-label="Expand sidebar category &#x27;编程语言&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/深度学习/"><span title="深度学习" class="categoryLinkLabel_W154">深度学习</span></a><button aria-label="Expand sidebar category &#x27;深度学习&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/docs/大模型的应用/"><span title="大模型的应用" class="categoryLinkLabel_W154">大模型的应用</span></a><button aria-label="Collapse sidebar category &#x27;大模型的应用&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/大模型的应用/模型社区/"><span title="模型社区" class="linkLabel_WmDU">模型社区</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/大模型的应用/上下文工程/"><span title="上下文工程" class="linkLabel_WmDU">上下文工程</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/大模型的应用/Agent开发/"><span title="Agent开发" class="linkLabel_WmDU">Agent开发</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/大模型的应用/检索增强/"><span title="检索增强" class="linkLabel_WmDU">检索增强</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/后端通识/"><span title="后端通识" class="categoryLinkLabel_W154">后端通识</span></a><button aria-label="Expand sidebar category &#x27;后端通识&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/浅尝辄止/"><span title="浅尝辄止" class="categoryLinkLabel_W154">浅尝辄止</span></a><button aria-label="Expand sidebar category &#x27;浅尝辄止&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/大模型的应用/"><span>大模型的应用</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">模型社区</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>模型社区</h1></header><p>本部分内容告诉你如何选择或者微调量化出功能及大小合适的模型并部署。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="开源模型">开源模型<a href="#开源模型" class="hash-link" aria-label="Direct link to 开源模型" title="Direct link to 开源模型" translate="no">​</a></h2>
<p>所谓“开源模型”，通常指公开以下两个核心部分：</p>
<ul>
<li>模型架构（Framework）：即网络结构设计，如 Transformer 层堆叠方式、注意力机制等；</li>
<li>参数权重（Weights）：训练完成后保存的数值，决定模型的实际能力。</li>
</ul>
<p>早期代表性开源模型如 LLaMA（Meta）、Qwen（通义）、ChatGLM（智谱）等，均完整发布架构代码和权重文件，推动了社区快速发展。这些模型的设计大多基于经典论文《Attention Is All You Need》提出的 Transformer 架构，成为当今绝大多数大模型的技术基石。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="模型架构">模型架构<a href="#模型架构" class="hash-link" aria-label="Direct link to 模型架构" title="Direct link to 模型架构" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="架构优化">架构优化<a href="#架构优化" class="hash-link" aria-label="Direct link to 架构优化" title="Direct link to 架构优化" translate="no">​</a></h4>
<p>在基础模型之上，各大厂商和研究机构进行了多样化改进：</p>
<ul>
<li>调整网络层数或注意力头数；</li>
<li>替换激活函数；</li>
<li>优化位置编码；</li>
<li>改进训练策略。</li>
</ul>
<p>每个厂商都有自己的优化想法，由此催生了 Qwen、ChatGLM 等一系列具有各自特色的衍生模型。</p>
<p>以通义千问（Qwen）为例，其经历了从 Qwen → Qwen-2 → Qwen-2.5 → Qwen3 的持续迭代。每一代升级都伴随着架构优化、训练数据扩充、上下文长度提升或推理效率增强，体现了技术的快速演进。</p>
<p>不同的架构显然是不同的开源模型，大模型抄袭与套皮往往指的就是架构一致。下面的表格列出了头部公司及其主要模型代号：</p>
<table><thead><tr><th style="text-align:left"><strong>国内</strong></th><th style="text-align:left"><strong>国外</strong></th></tr></thead><tbody><tr><td style="text-align:left">阿里巴巴: 通义千问 (Qwen)/通义万象(Wan)</td><td style="text-align:left">谷歌: Gemini/Veo</td></tr><tr><td style="text-align:left">字节跳动: 豆包 (coze)/即梦(Seedream)</td><td style="text-align:left">OpenAI: GPT/Sora</td></tr><tr><td style="text-align:left">深度求索: DeepSeek</td><td style="text-align:left">Anthropic: Claude</td></tr><tr><td style="text-align:left">智谱: 智谱清言 (GLM)</td><td style="text-align:left">xAI: Grok</td></tr><tr><td style="text-align:left">腾讯: 混元 (Hunyuan)</td><td style="text-align:left">Meta: Llama</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="参数权重">参数权重<a href="#参数权重" class="hash-link" aria-label="Direct link to 参数权重" title="Direct link to 参数权重" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="参数多少">参数多少<a href="#参数多少" class="hash-link" aria-label="Direct link to 参数多少" title="Direct link to 参数多少" translate="no">​</a></h4>
<p>模型的参数量是影响性能的关键因素之一，常见规格包括：7B（70亿）、14B（140亿）、72B（720亿）等。</p>
<p>一般来说：参数越大，语言理解与生成能力越强，但对算力、显存要求也更高；参数越小，推理速度快、部署门槛低，适合端侧或边缘设备。</p>
<p>因此，厂商常发布同一架构下的多个参数版本，如 Qwen3-7B、Qwen3-14B、Qwen3-72B，满足不同场景需求。</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>随着大模型走向商业化闭源，部分厂商开始淡化甚至隐藏具体参数信息，转而采用更具用户  体验导向的命名方式。</p><ul>
<li>Qwen3-14B 可能对应名称为 Qwen3-Fast</li>
<li>Qwen3-72B 可能对应名称为 Qwen3-High</li>
</ul></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="权重大小">权重大小<a href="#权重大小" class="hash-link" aria-label="Direct link to 权重大小" title="Direct link to 权重大小" translate="no">​</a></h4>
<p>在预训练模型基础上，将权重大小使用特定领域数据进行进一步训练调整，称为微调（fine-tuning），用于提升模型在某类任务上的表现。微调后的模型也算一个新的开源模型。</p>
<ul>
<li>全量微调（Full Fine-tuning）：更新全部参数，适合有大量高质量数据的场景；</li>
<li>LoRA 微调（Low-Rank Adaptation）：仅训练低秩矩阵，冻结主干权重，显著降低计算开销。</li>
</ul>
<p>通过微调，可得到面向特定任务的专用模型，例如：</p>
<ul>
<li>Qwen3-Coder：擅长代码生成；</li>
<li>Qwen3-Math：专精数学推理</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="权重精度">权重精度<a href="#权重精度" class="hash-link" aria-label="Direct link to 权重精度" title="Direct link to 权重精度" translate="no">​</a></h4>
<p>将权重精度从高精度浮点数（如 float64）转换为低精度表示：</p>
<p>float32 → float16 / bf16 → int8 / int4
虽然会带来轻微精度损失（如 99.99 → 99），但能显著：</p>
<ul>
<li>减少模型体积；</li>
<li>降低内存占用；</li>
<li>提升推理速度；</li>
<li>降低部署门槛。</li>
</ul>
<p>这个技术称为：模型量化（quantization）</p>
<p>量化后的模型也算新的开源模型。通常在名称中标注精度类型，例如：-fp16、-int8、-g16 等。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="演化过程">演化过程<a href="#演化过程" class="hash-link" aria-label="Direct link to 演化过程" title="Direct link to 演化过程" translate="no">​</a></h3>
<p>开源模型的完整演化过程可以用下图表示：</p>
<!-- -->
<p>这四类演化中，架构是最重要的，目前可以分为三代（代表模型不是最早模型，而是知名度最高的模型）：</p>
<table><thead><tr><th style="text-align:left">代际</th><th style="text-align:left">时间段</th><th style="text-align:left">特征</th><th style="text-align:left">代表模型</th></tr></thead><tbody><tr><td style="text-align:left">第一代</td><td style="text-align:left">2020–2022</td><td style="text-align:left">传统大模型应用</td><td style="text-align:left">GPT-3.5</td></tr><tr><td style="text-align:left">第二代</td><td style="text-align:left">2023–2024</td><td style="text-align:left">原生多模态应用</td><td style="text-align:left">GPT-4o</td></tr><tr><td style="text-align:left">第三代</td><td style="text-align:left">2025–</td><td style="text-align:left">高效架构（MoE）+ 智能体（Agent）范式</td><td style="text-align:left">Claude-4.5</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Qwen 3-MAX 使用的也是MoE架构。官方称其参数超过1T（1000B）远超最大开源模型 Llama 3.1（405B ）是国内最顶级的第三代模型之一。</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="大模型社区">大模型社区<a href="#大模型社区" class="hash-link" aria-label="Direct link to 大模型社区" title="Direct link to 大模型社区" translate="no">​</a></h2>
<p>大模型社区是指围绕大型深度学习模型构建的开放协作平台和生态系统，除了开源模型还提供：数据集、教程、体验等功能。这些社区由研究人员、开发者、数据科学家、工程师及爱好者组成，他们共同致力于大模型的研究、开发、优化和应用。</p>
<p>社区具有明显的马太效应，即头部效应明显，头部模型拥有最多的资源，最新的技术，最多的用户。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="开源模型推理">开源模型推理<a href="#开源模型推理" class="hash-link" aria-label="Direct link to 开源模型推理" title="Direct link to 开源模型推理" translate="no">​</a></h3>
<p>Hugging Face 是国外最著名的开源社区，提供了 <code>transformers</code> 库，可以方便地下载模型，地址：<a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p>
<p>魔搭社区是阿里达摩院推出的开源社区，基于中国网络环境，<code>modelscope</code> 库，对标 Hugging Face 的 <code>transformers</code> 库。地址：<a href="https://www.modelscope.cn/" target="_blank" rel="noopener noreferrer">https://www.modelscope.cn/</a></p>
<p>以 Qwen 模型为例，下面展示如何使用 <code>transformers</code> 或 <code>modelscope</code> 库进行推理。其中<code>model_name</code>为模型地址。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">types </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;huggingface&quot;</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># 模型社区选择：&quot;huggingface&quot; 或 &quot;modelscope&quot;</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">if</span><span class="token plain"> types </span><span class="token operator" style="color:rgb(0, 0, 0)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;huggingface&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> AutoModelForCausalLM</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> AutoTokenizer</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">elif</span><span class="token plain"> types </span><span class="token operator" style="color:rgb(0, 0, 0)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;modelscope&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> modelscope </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> AutoModelForCausalLM</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> AutoTokenizer</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">model_size </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;3B&quot;</span><span class="token plain">  </span><span class="token comment" style="color:rgb(0, 128, 0)"># 3B 7B 14B 32B</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">model_name </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token string-interpolation string" style="color:rgb(163, 21, 21)">f&quot;Qwen/Qwen2.5-</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string-interpolation interpolation">model_size</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token string-interpolation string" style="color:rgb(163, 21, 21)">-Instruct&quot;</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">model </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> AutoModelForCausalLM</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model_name</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> torch_dtype</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;auto&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> device_map</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;auto&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">tokenizer </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> AutoTokenizer</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model_name</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">while</span><span class="token plain"> </span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">input</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;输入你的问题: &quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">if</span><span class="token plain"> prompt </span><span class="token operator" style="color:rgb(0, 0, 0)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;退出&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token keyword" style="color:rgb(0, 0, 255)">break</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    messages </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;system&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;你是一个AI助手，由阿里巴巴云创建。你是一个乐 于助人的助手。你总是以中文回答问题。&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;role&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;user&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(163, 21, 21)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> prompt</span><span class="token punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    text </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">apply_chat_template</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">messages</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> tokenize</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">False</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> add_generation_prompt</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    model_input </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">text</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> return_tensors</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token string" style="color:rgb(163, 21, 21)">&quot;pt&quot;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">to</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">device</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    generated_ids </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">generate</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token operator" style="color:rgb(0, 0, 0)">**</span><span class="token plain">model_input</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> max_new_tokens</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token number" style="color:rgb(9, 134, 88)">512</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    generated_ids </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token plain">output</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token builtin" style="color:rgb(0, 112, 193)">len</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(0, 0, 255)">for</span><span class="token plain"> input_ids</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> output </span><span class="token keyword" style="color:rgb(0, 0, 255)">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(0, 112, 193)">zip</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">model_input</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> generated_ids</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    response </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> tokenizer</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">batch_decode</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">generated_ids</span><span class="token punctuation" style="color:rgb(4, 81, 165)">,</span><span class="token plain"> skip_special_tokens</span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token number" style="color:rgb(9, 134, 88)">0</span><span class="token punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token plain">response</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span></span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="开源数据集">开源数据集<a href="#开源数据集" class="hash-link" aria-label="Direct link to 开源数据集" title="Direct link to 开源数据集" translate="no">​</a></h3>
<p>除了开源模型，还有开源数据集。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token keyword" style="color:rgb(0, 0, 255)">from</span><span class="token plain"> modelscope</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">msdatasets </span><span class="token keyword" style="color:rgb(0, 0, 255)">import</span><span class="token plain"> MsDataset</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">dataset </span><span class="token operator" style="color:rgb(0, 0, 0)">=</span><span class="token plain"> MsDataset</span><span class="token punctuation" style="color:rgb(4, 81, 165)">.</span><span class="token plain">load</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string" style="color:rgb(163, 21, 21)">&#x27;swift/Chinese-Qwen3-235B-2507-Distill-data-110k-SFT&#x27;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:rgb(0, 0, 255)">print</span><span class="token punctuation" style="color:rgb(4, 81, 165)">(</span><span class="token string-interpolation string" style="color:rgb(163, 21, 21)">f&#x27;dataset[0]: </span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">{</span><span class="token string-interpolation interpolation">dataset</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">[</span><span class="token string-interpolation interpolation number" style="color:rgb(9, 134, 88)">0</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">]</span><span class="token string-interpolation interpolation punctuation" style="color:rgb(4, 81, 165)">}</span><span class="token string-interpolation string" style="color:rgb(163, 21, 21)">&#x27;</span><span class="token punctuation" style="color:rgb(4, 81, 165)">)</span></span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="模型评测">模型评测<a href="#模型评测" class="hash-link" aria-label="Direct link to 模型评测" title="Direct link to 模型评测" translate="no">​</a></h2>
<p>为了更好的了解当下模型性能，我们列举了几个模型排名网站，这些网站通过主观测试（人工盲选较优模型）或者客观测试（自动通过含有答案的测试对比准确度），对模型进行排名，并给出排名结果。</p>
<ul>
<li>Hugging Face 模型排名：<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></li>
<li>lmarena排名：<a href="https://lmarena.ai/" target="_blank" rel="noopener noreferrer">https://lmarena.ai/</a></li>
</ul>
<p>那么如何自动化测评得出排名？</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="自动化评测">自动化评测<a href="#自动化评测" class="hash-link" aria-label="Direct link to 自动化评测" title="Direct link to 自动化评测" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="考试评测客观题与标准化能力测试">&quot;考试&quot;评测：客观题与标准化能力测试<a href="#考试评测客观题与标准化能力测试" class="hash-link" aria-label="Direct link to &quot;考试&quot;评测：客观题与标准化能力测试" title="Direct link to &quot;考试&quot;评测：客观题与标准化能力测试" translate="no">​</a></h4>
<p>这是一种最基础且高效的评测方法，类似于给模型进行一场&quot;标准化考试&quot;。评测者提供包含多选题或客观题的数据集，模型给出答案，然后通过脚本或工具自动比对答案，计算分数。这种方法的优点在于评测过程客观、可重复，且成本低廉。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="核心基准测试">核心基准测试<a href="#核心基准测试" class="hash-link" aria-label="Direct link to 核心基准测试" title="Direct link to 核心基准测试" translate="no">​</a></h5>
<p><strong>MMLU (Massive Multitask Language Understanding)</strong>：这是一个大规模的英文多任务测试，包含来自人文、社会科学、硬科学等57个领域的选择题。要在这个测试中获得高分，模型必须具备广泛的世界知识和强大的问题解决能力，而非仅仅是单一领域的专家。</p>
<p><strong>C-Eval</strong>：作为MMLU的中文版，C-Eval是专门为评估语言模型在中文语境下的知识和推理能力而设计的综合性基准。它包含了13,948道多选题，跨越52个学科和4个难度级别，其中许多问题都具有中国特定的文化和常识背景。</p>
<p><strong>GSM8K</strong>：这是一个由小学数学应用题组成的数据集，主要用于评估模型在数学方面的逻辑推理能力。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="如何得出评分">如何得出评分<a href="#如何得出评分" class="hash-link" aria-label="Direct link to 如何得出评分" title="Direct link to 如何得出评分" translate="no">​</a></h5>
<p>这类评测的核心指标通常是准确率（Accuracy），即模型答对的题目数占总题目数的百分比。例如，在C-Eval的100道题中，如果模型答对了80道，那么它的准确率就是80%。在评测中，准确率分数越高越好。</p>
<p>然而，仅仅依赖这些标准化基准也存在挑战。随着模型能力的提升，业界开始担忧模型是否仅仅是通过对训练数据集的&quot;死记硬背&quot;来获得高分，而非真正具备推理能力。例如，GPT-4在C-Eval Hard等榜单上的表现优于其他模型，但其能力来源是否仅仅是记忆，引发了新的讨论。为了解决这种&quot;刷榜&quot;和数据偏见的问题，研究者们开始构建新的、更具挑战性的评测基准，如GSM-Symbolic和GSM8K-Platinum。这些新的基准通过修正原有数据集中的标签噪音和生成更多样的变体问题，来更准确地衡量模型的真实能力。</p>
<p>这种模型与评测基准之间的博弈，恰恰体现了评测工作的动态演进本  质。一个优秀的评测工程师不仅要懂得如何使用现有的工具，更要具备敏锐的洞察力，能够识别现有评测的局限性，并为设计新一代、更可靠的评测体系提供宝贵的见解。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="创作评测评估文本生成质量">&quot;创作&quot;评测：评估文本生成质量<a href="#创作评测评估文本生成质量" class="hash-link" aria-label="Direct link to &quot;创作&quot;评测：评估文本生成质量" title="Direct link to &quot;创作&quot;评测：评估文本生成质量" translate="no">​</a></h4>
<p>相较于有唯一正确答案的客观题，评估模型生成开放性、非确定性文本（如摘要、翻译、创作等）的能力更具挑战性。自动化评测通过复杂的算法，衡量模型生成文本与人工参考文本之间的相似度，以量化其质量。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="核心指标详解">核心指标详解<a href="#核心指标详解" class="hash-link" aria-label="Direct link to 核心指标详解" title="Direct link to 核心指标详解" translate="no">​</a></h5>
<p><strong>BLEU (Bilingual Evaluation Understudy)</strong></p>
<ul>
<li><strong>核心思想</strong>：BLEU基于n-gram（连续词组）重合度，来衡量模型生成的文本与参考答案有多大的词汇重叠。</li>
<li><strong>如何得出评分</strong>：它计算模型生成文本中与参考答案重合的n-gram数量。为了防止模型只生成少量高频词以获得高分，BLEU还引入了简短惩罚（Brevity Penalty）。如果生成文本比参考答案短，分数会降低。</li>
<li><strong>计算举例</strong>：<!-- -->
<ul>
<li>参考答案：&quot;The cat sat on the mat.&quot;</li>
<li>模型生成：&quot;The cat is sitting on the mat.&quot;</li>
<li>BLEU算法会识别出&quot;the&quot;, &quot;cat&quot;, &quot;on&quot;, &quot;the&quot;, &quot;mat&quot;等单词的重合。</li>
</ul>
</li>
<li><strong>分数解读</strong>：分数范  围在0到1之间，分数越高越好。但需要注意的是，单纯的高分可能无法反映生成文本的语义和语法正确性，因为BLEU不考虑词序和同义词。</li>
</ul>
<p><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong></p>
<ul>
<li><strong>核心思想</strong>：与BLEU强调精确率不同，ROUGE更关注召回率，即模型生成的文本捕获了多少参考文本中的关键信息。</li>
<li><strong>如何得出评分</strong>：<!-- -->
<ul>
<li>ROUGE-N：基于n-gram重合度计算，其召回率公式为：ROUGE−N Recall=参考答案的n−gram总数重合的n−gram数​。</li>
<li>ROUGE-L：基于最长公共子序列（LCS）。它寻找两个文本中最长的、顺序一致但不要求连续的单词序列。这使其能更好地衡量句子的整体结构和信息流。</li>
</ul>
</li>
<li><strong>分数解读</strong>：分数范围同样在0到1之间，分数越高越好。ROUGE特别适用于评估文本摘要和问答任务中关键信息的完整性。</li>
</ul>
<p><strong>METEOR (Metric for Evaluation of Translation with Explicit Ordering)</strong></p>
<ul>
<li><strong>核心思想</strong>：METEOR被认为是比BLEU和ROUGE更全面的指标。它综合了精确率和召回率，并加入了词形还原（stemming）和同义词匹配的功能。这使得它能更好地处理不同形式但意思相同的词语，能更接近人类的判断。</li>
<li><strong>如何得出评分</strong>：它计算精确率和召回率的调和平均值，并加入一个基于&quot;分块&quot;（chunks）数量的惩罚项，来反映词语的乱序程度。</li>
<li><strong>分数解读</strong>：分数范围在0到1之间，分数越高越好。研究表明，METEOR与人类判断的相关性通常更高。</li>
</ul>
<p>尽管这些自动化指标高效，但它们无法完全捕捉模型的流畅性、逻辑性和观点表达等深层能力。因此，在实际工作中，评测者通常会采用混合评测方式，将自动化评测与人 工评测（通过专家打分）和大模型辅助评测（利用GPT-4等强大模型作为&quot;裁判员&quot;）结合起来。一个专业的评测工程师不会只依赖某一个指标，而是能根据具体任务，选择最合适的指标组合，甚至为特定业务需求设计独特的评测体系，例如在代码生成领域使用Pass@k指标来评估代码的功能正确性。这凸显了评测工作的价值在于&quot;组合&quot;与策略。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="多模态自动化评测">多模态自动化评测<a href="#多模态自动化评测" class="hash-link" aria-label="Direct link to 多模态自动化评测" title="Direct link to 多模态自动化评测" translate="no">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="文生图t2i评测图像的视觉与语义衡量">文生图（T2I）评测：图像的&quot;视觉&quot;与&quot;语义&quot;衡量<a href="#文生图t2i评测图像的视觉与语义衡量" class="hash-link" aria-label="Direct link to 文生图（T2I）评测：图像的&quot;视觉&quot;与&quot;语义&quot;衡量" title="Direct link to 文生图（T2I）评测：图像的&quot;视觉&quot;与&quot;语义&quot;衡量" translate="no">​</a></h4>
<p>多模态模型，尤其是文生图（Text-to-Image, T2I）模型，其评测挑战在于需要同时衡量两个关键维度：生成的图像质量（是否真实、美观）以及与输入文本的一致性（是否准确反映提示词）。</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="核心指标详解-1">核心指标详解<a href="#核心指标详解-1" class="hash-link" aria-label="Direct link to 核心指标详解" title="Direct link to 核心指标详解" translate="no">​</a></h5>
<p><strong>FID (Fréchet Inception Distance)</strong></p>
<ul>
<li><strong>核心思想</strong>：FID是评估图像生成模型质量和多样性的黄金标准。它不直接比较单个生成的图像与真实图像，而是通过计算生成图像的群体分布与真实图 像的群体分布之间的距离。</li>
<li><strong>如何得出评分</strong>：FID利用一个预训练的图像分类模型（如Inception V3）来提取图像的高维特征。它将真实图像和生成图像的特征向量分别建模为多元正态分布，然后计算这两个分布之间的弗雷谢距离。</li>
<li><strong>分数解读</strong>：FID分数越低越好。一个完美的模型，其生成的图像分布与真实图像分布完全一致，FID为0。一个形象的比喻是&quot;遛狗&quot;：如果你的模型能生成各种各样的狗（多样性），且每只狗都栩栩如生（真实性），那么它的FID分数就会很低。反之，如果它只生成一种狗，或者生成的狗都模糊不清，那么FID就会很高。</li>
</ul>
<p><strong>CLIP Score (Contrastive Language-Image Pretraining Score)</strong></p>
<ul>
<li><strong>核心思想</strong>：CLIP Score专门评估生成图像与输入文本描述的匹配程度，即&quot;图文一致性&quot;。</li>
<li><strong>如何得出评分</strong>：CLIP模型将图像和文本映射到同一个嵌入空间，使得语义相关的图文对在此空间中距离相近。CLIP Score就是通过计算图像向量和文本向量之间的余弦相似度来衡量它们的语义相关性。</li>
<li><strong>分数解读</strong>：分数范围通常在-1到1，分数越高越好。</li>
</ul>
<p><strong>CIDEr (Consensus-based Image Description Evaluation)</strong></p>
<ul>
<li><strong>核心思想</strong>：该指标尤其适用于图像描述生成任务。它通过衡量生成的描述与多个人工参考描述之间的共识程度来打分，并使用TF-IDF（词频-逆文档频率）加权，给那些罕见但重要的词语更高的权重。</li>
<li><strong>分数解读</strong>：分数越高越好。</li>
</ul>
<p><strong>SPICE (Semantic Propositional Image Caption Evaluation)</strong></p>
<ul>
<li><strong>核心思想</strong>：SPICE更进一步，它通过**语义图（semantic graph）**来评估生成描述与参  考描述的语义相似度，而不仅仅是词语重叠。它关注描述中物体、属性和它们之间的关系，因此对同义词和改写有更强的鲁棒性。</li>
<li><strong>分数解读</strong>：分数越高越好。</li>
</ul>
<p>图像生成模型的评测是多维度的，没有单一指标能完美涵盖所有方面。FID关注图像群体的分布，CLIP Score关注图文匹配，而CIDEr/SPICE关注描述的语义质量。同时，这些自动化指标也存在局限性：FID依赖于特定的预训练模型，计算成本高昂；CLIP Score虽然高效，但可能无法捕捉到细节（如艺术家风格），导致评测不准确。这些局限性推动了行业对更多元化、更精细的评测方法和榜单的探索，并形成了评测工作的**&quot;全栈&quot;**属性。一个顶尖的AI评测工程师不仅要理解这些指标，还要能根据项目需求，选择和组合不同的指标，甚至进行众包评测，从而为模型优化提供最精准、最具价值的反馈。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="微调与量化">微调与量化<a href="#微调与量化" class="hash-link" aria-label="Direct link to 微调与量化" title="Direct link to 微调与量化" translate="no">​</a></h2>
<p>模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>把模型想象为一 个固定容量的大脑，脑容量是固定的，通过微调让它对A印象深刻，它就会淡忘B。</p></div></div>
<p>LLaMA-Factory 是基于 LLaMA 的模型微调框架，支持 LoRA 微调、冻结层微调、全量微调。既可以通过 WebUI 微调，也可以通过命令行微调。</p>
<p>项目地址：<a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener noreferrer">https://github.com/hiyouga/LLaMA-Factory</a></p>
<p>安装</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">cd LLaMA-Factory</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">pip install --no-deps -e .</span><br></span></code></pre></div></div>
<p>启动 WebUI</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">llamafactory-cli webui</span><br></span></code></pre></div></div>
<p>微调有三种方法：LoRA 微调、冻结层微调、全量微调。</p>
<p>以微调一个 <strong>14B 参数</strong>的大型语言模型为例，以下是对三种常见微调方法的硬件需求和大致时间的估算：</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-微调">LoRA 微调<a href="#lora-微调" class="hash-link" aria-label="Direct link to LoRA 微调" title="Direct link to LoRA 微调" translate="no">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：~24GB 显存以上即可（如 NVIDIA RTX 3090 或 A100）。LoRA 主要通过冻结大部分模型参数，仅在少量层中插入低秩适配模块，因此显存需求较低。</li>
<li><strong>GPU 数量</strong>：单卡或 2 卡即可应付中等规模的微调任务。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>（如数十万条数据，100k steps 以内）：几个小时到 1 天。</li>
<li><strong>大规模数据</strong>（如百万条数据，300k steps）：2-3 天。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>高效，显存需求低，适合个人开发者或中小型实验室。</li>
<li>微调后的模型参数（LoRA 插件）仅几百 MB，方便存储和共享。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="冻结层微调">冻结层微调<a href="#冻结层微调" class="hash-link" aria-label="Direct link to 冻结层微调" title="Direct link to 冻结层微调" translate="no">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：需要 48GB 以上显存（如 A6000 或 80GB A100）。冻结大部分模型参数，微调后几层或部分特定模块。</li>
<li><strong>GPU 数量</strong>：1-4 张高性能 GPU。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>：1-2 天。</li>
<li><strong>大规模数据</strong>：3-5 天。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>显存需求较低，性能适中。</li>
<li>微调时间比全量微调短，但仍需较多资源。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="全量微调">全量微调<a href="#全量微调" class="hash-link" aria-label="Direct link to 全量微调" title="Direct link to 全量微调" translate="no">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：需 80GB 显存以上的高性能 GPU（如 NVIDIA A100 或 H100）。14B 参数模型通常需要 4 卡或更多 GPU 的分布式训练。</li>
<li><strong>GPU 数量</strong>：4-8 张 A100（或等效）GPU。对于超大模型，可能需要更多。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>：2-3 天。</li>
<li><strong>大规模数据</strong>：5 天到 1 周甚至更长。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>微调灵活性最高，可以针对特定任务完全优化模型。</li>
<li>模型质量可能略高于 LoRA 和冻结层微调。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>显存需求高，成本昂贵。</li>
<li>微调后模型体积巨大，通常需要数百 GB 存储。</li>
</ul>
<p>有些框架可以以时间换空间，例如使用梯度检查点：在显存不足时保存中间计算结果，降低瞬时显存需求。或者以精度换空间，例如使用混合精度训练：可以显著减少显存需求和训练时间。有兴趣可以自行搜索。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="量化">量化<a href="#量化" class="hash-link" aria-label="Direct link to 量化" title="Direct link to 量化" translate="no">​</a></h3>
<p>随着语言模型规模的不断增大，其训练的难度和成本已成为共识。 而随着用户数量的增加，模型推理的成本也在不断攀升，甚至可能成为限制模型部署的首要因素。 因此，我们需要对模型进行压缩以加速推理过程，而模型量化是其中一种有效的方法。</p>
<p>大语言模型的参数通常以高精度浮点数存储，这导致模型推理需要大量计算资源。 量化技术通过将高精度数据类型存储的参数转换为低精度数据类型存储， 可以在不改变模型参数量和架构的前提下加速推理过程。这种方法使得模型的部署更加经济高效，也更具可行性。</p>
<p>浮点数一般由 3 部分组  成：符号位、指数位和尾数位。指数位越大，可表示的数字范围越大。尾数位越大、数字的精度越高。</p>
<p>量化可以根据何时量化分为：后训练量化和训练感知量化，也可以根据量化参数的确定方式分为：静态量化和动态量化。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="qat-感知量化训练">QAT: 感知量化训练<a href="#qat-感知量化训练" class="hash-link" aria-label="Direct link to QAT: 感知量化训练" title="Direct link to QAT: 感知量化训练" translate="no">​</a></h4>
<p>在训练感知量化（QAT, <strong>Quantization-Aware Training</strong>）中，模型一般在预训练过程中被量化，然后又在训练数据上再次微调，得到最后的量化模型。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ptq-后训练量化">PTQ: 后训练量化<a href="#ptq-后训练量化" class="hash-link" aria-label="Direct link to PTQ: 后训练量化" title="Direct link to PTQ: 后训练量化" translate="no">​</a></h4>
<p>后训练量化（PTQ, <strong>Post-Training Quantization</strong>）一般指在模型预训练完成后，基于校准数据集（<strong>calibration dataset</strong>）确定量化参数进而对模型进行量化。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gptq-分组精度调优量化">GPTQ: 分组精度调优量化<a href="#gptq-分组精度调优量化" class="hash-link" aria-label="Direct link to GPTQ: 分组精度调优量化" title="Direct link to GPTQ: 分组精度调优量化" translate="no">​</a></h4>
<p>GPTQ（<strong>Group-wise Precision Tuning Quantization</strong>）是一种静态的后训练量化技术。<br>
<!-- -->“静态”指的是预训练模型一旦确定，经过量化后量化参数不再更改。<br>
<strong>GPTQ 的特点</strong>：</p>
<ul>
<li>将 fp16 精度模型量化为 4-bit</li>
<li>节省约 75% 的显存</li>
<li>大幅提高推理速度</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> TechxGenus/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">GPTQ</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="awq-激活感知层量化">AWQ: 激活感知层量化<a href="#awq-激活感知层量化" class="hash-link" aria-label="Direct link to AWQ: 激活感知层量化" title="Direct link to AWQ: 激活感知层量化" translate="no">​</a></h4>
<p>AWQ（<strong>Activation-Aware Layer Quantization</strong>）是一种静态的后训练量化技术。其核心思想为：少部分重要权重保持不被量化，以维持模型性能。<br>
<strong>AWQ 的优势</strong>：</p>
<ul>
<li>所需校准数据集更小</li>
<li>在指令微调和多模态模型上表现良好</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> TechxGenus/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">AWQ</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="aqlm-语言模型加法量化">AQLM: 语言模型加法量化<a href="#aqlm-语言模型加法量化" class="hash-link" aria-label="Direct link to AQLM: 语言模型加法量化" title="Direct link to AQLM: 语言模型加法量化" translate="no">​</a></h4>
<p>AQLM（<strong>Additive Quantization of Language Models</strong>）是一种仅对模型权重进行量化的 PTQ 方法。<br>
<strong>特点</strong>：</p>
<ul>
<li>在 2-bit 量化下达到了最佳性能</li>
<li>在 3-bit 和 4-bit 量化下同样表现优异</li>
<li>2-bit 量化适用于低显存部署大模型</li>
</ul>
<p>尽管 AQLM 的推理速度提升并不显著，但其极低的显存占用具有很高实用价值。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="oftq-动态后训练量化">OFTQ: 动态后训练量化<a href="#oftq-动态后训练量化" class="hash-link" aria-label="Direct link to OFTQ: 动态后训练量化" title="Direct link to OFTQ: 动态后训练量化" translate="no">​</a></h4>
<p>OFTQ（<strong>On-the-fly Quantization</strong>）无需校准数据集，直接在推理阶段进行动态量化。<br>
<strong>特点</strong> ：</p>
<ul>
<li>无需校准数据集</li>
<li>能够在推理阶段动态量化，保持性能</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">llama/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token key atrule">quantization_bit</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(9, 134, 88)">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token key atrule">quantization_method</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> bitsandbytes  </span><span class="token comment" style="color:rgb(0, 128, 0)"># 可选: [bitsandbytes (4/8), hqq (2/3/4/5/6/8), eetq (8)]</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bitsandbytes-动态后训练量化">bitsandbytes: 动态后训练量化<a href="#bitsandbytes-动态后训练量化" class="hash-link" aria-label="Direct link to bitsandbytes: 动态后训练量化" title="Direct link to bitsandbytes: 动态后训练量化" translate="no">​</a></h4>
<p>区别于 GPTQ，<strong>bitsandbytes</strong> 是一种动态的后训练量化技术。<br>
<strong>特点</strong>：</p>
<ul>
<li>支持大于 1B 的模型量化</li>
<li>在 8-bit 量化后，性能损失极小</li>
<li>能够节省约 50% 的显存</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="hqq-半二次量化">HQQ: 半二次量化<a href="#hqq-半二次量化" class="hash-link" aria-label="Direct link to HQQ: 半二次量化" title="Direct link to HQQ: 半二次量化" translate="no">​</a></h4>
<p>HQQ（<strong>Half-Quadratic Quantization</strong>）在准确度和速度之间取得平衡。<br>
<strong>特点</strong>：</p>
<ul>
<li>不需要校准阶段</li>
<li>推理速度极快</li>
<li>准确度与需要校准数据的方法相当</li>
</ul>
<p>HQQ 是一种动态的后训练量化方法，适合需要快速推理且性能敏感的场景。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="eetq-高效变换器量化">EETQ: 高效变换器量化<a href="#eetq-高效变换器量化" class="hash-link" aria-label="Direct link to EETQ: 高效变换器量化" title="Direct link to EETQ: 高效变换器量化" translate="no">​</a></h4>
<p>EETQ（<strong>Easy and Efficient Quantization for Transformers</strong>）是一种只对模型权重进行量化的 PTQ 方法。<br>
<strong>特点</strong>：</p>
<ul>
<li>速度快</li>
<li>简单易用</li>
</ul>
<p>EETQ 特别适合对性能和实现复杂度都有较高要求的用户。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="模型本地部署">模型本地部署<a href="#模型本地部署" class="hash-link" aria-label="Direct link to 模型本地部署" title="Direct link to 模型本地部署" translate="no">​</a></h2>
<p>截至2025年6月,大模型最佳实践依然是本地运行。大型和小型算力平台易用性与性价比均不高。</p>
<p>小型算力平台例如：无问芯穹，需要先企业认证才能开始模型训练与微调。</p>
<p>大型算例平台例如：阿里云百炼，无需企业认证，但是200MB的训练语料就需要加载80分钟，数据还需要分割后再上传。200MB语料下，训练<code>Qwen2.5 7b</code>一轮训练就需要4小时，约18元。换算后：1个G的语料，训练<code>Qwen2.5 7b</code>一轮训练就需要20小时，约90元。成本是租算力1倍往上。</p>
<p>社区上默认的部署方式往往是用来测试，生产环境下我们往往需要：并发高、延迟低、占用小。同时兼顾不同的底层硬件。主流部署框架是 Ollama 和 VLLM。</p>
<table><thead><tr><th style="text-align:left">维度</th><th style="text-align:left">Ollama</th><th style="text-align:left">VLLM</th></tr></thead><tbody><tr><td style="text-align:left"><strong>官网</strong></td><td style="text-align:left"><a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">https://ollama.com/</a></td><td style="text-align:left"><a href="https://vllm.ai/" target="_blank" rel="noopener noreferrer">https://vllm.ai/</a></td></tr><tr><td style="text-align:left"><strong>GitHub</strong></td><td style="text-align:left"><a href="https://github.com/ollama/ollama" target="_blank" rel="noopener noreferrer">https://github.com/ollama/ollama</a></td><td style="text-align:left"><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a></td></tr><tr><td style="text-align:left"><strong>核心功能</strong></td><td style="text-align:left">模型管理和推理框架，支持快速加载、切换模型</td><td style="text-align:left">高性能推理引擎，优化 Transformer 模型推理</td></tr><tr><td style="text-align:left"><strong>主要特点</strong></td><td style="text-align:left">易用的命令行工 具，支持多个预训练模型</td><td style="text-align:left">动态批次合并（dynamic batching），高吞吐量推理</td></tr><tr><td style="text-align:left"><strong>性能</strong></td><td style="text-align:left">适中，重点在于易用性</td><td style="text-align:left">高，专为分布式和高吞吐量推理优化</td></tr><tr><td style="text-align:left"><strong>支持硬件</strong></td><td style="text-align:left">CPU/GPU</td><td style="text-align:left">GPU</td></tr><tr><td style="text-align:left"><strong>异步支持</strong></td><td style="text-align:left">支持多任务异步加载模型</td><td style="text-align:left">原生支持，面向大规模分布式推理设计</td></tr><tr><td style="text-align:left"><strong>部署难度</strong></td><td style="text-align:left">低，支持简单命令行部署</td><td style="text-align:left">中，需要配置分布式推理环境</td></tr><tr><td style="text-align:left"><strong>内存优化</strong></td><td style="text-align:left">支持基础内存管理</td><td style="text-align:left">通过动态批次和显存优化提高吞吐量</td></tr><tr><td style="text-align:left"><strong>适用场景</strong></td><td style="text-align:left">小型项目、模型快速切换、开发测试</td><td style="text-align:left">高性能推理、分布式推理、服务大规模用户</td></tr><tr><td style="text-align:left"><strong>对 Python 的支持</strong></td><td style="text-align:left">易于集成到 Python 应用中，支持 REST API</td><td style="text-align:left">强支持，直接集成到 Python 项目</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ollama示例">ollama示例<a href="#ollama示例" class="hash-link" aria-label="Direct link to ollama示例" title="Direct link to ollama示例" translate="no">​</a></h3>
<p>示例1<!-- -->:ollama<!-- -->部署huggingface模型</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">ollama run hf.co/{username}/{reponame}:latest</span></span><br></span></code></pre></div></div>
<p>示例2:运行最新的模型</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest</span></span><br></span></code></pre></div></div>
<p>示例3:运行特定的量化模型</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:Q8_0</span></span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vllm示例">vllm示例<a href="#vllm示例" class="hash-link" aria-label="Direct link to vllm示例" title="Direct link to vllm示例" translate="no">​</a></h3>
<p>示例1<!-- -->:vllm<!-- -->部署huggingface模型</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123</span></span><br></span></code></pre></div></div>
<p>示例2：vllm部署本地模型，调用8卡推理</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">vllm serve /home/ly/qwen2.5/Qwen2.5-32B-Instruct/ --tensor-parallel-size 8 --dtype auto --api-key 123 --gpu-memory-utilization 0.95 --max-model-len 27768  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2.5-32B-Instruct --kv-cache-dtype fp8_e5m2</span></span><br></span></code></pre></div></div>
<p>示例3：vllm部署本地模型，指定某块GPU运行模型</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#000000"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">CUDA_VISIBLE_DEVICES=2 vllm serve /home/ly/qwen2.5/Qwen2-VL-7B-Instruct --dtype auto --tensor-parallel-size 1 auto --api-key 123 --gpu-memory-utilization 0.5 --max-model-len 5108  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2-VL-7B-Instruct --port 1236</span></span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Vllm不支持启动一个服务就可以随机切换其他模型（ollama支持）。</p><p>通常需要为每一个模型单独运行一次vllm命令，并且每个模型都要提供不同的端口，比如他默认的是8000端口，而我上一个命令使用的是1236端口</p></div></div><div class="margin-vert--lg" style="display:flex;align-items:center;justify-content:center"><a class="button button--link" href="https://github.com/jiangyangcreate/jiangyangcreate.github.io/blob/main/docs/docs/大模型的应用/模型社区.mdx" target="_blank" rel="noreferrer noopener" aria-label="edit on github" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 512 512" height="20" width="20" style="margin-right:7px"><g transform="translate(0.000000,512.000000) scale(0.100000,-0.100000)" fill="url(#grad1)" stroke="none"><path d="M2384 5060 c-1012 -69 -1891 -738 -2235 -1703 -53 -146 -102 -349 -125 -512 -23 -157 -25 -496 -5 -655 102 -816 575 -1526 1287 -1932 164 -94 418 -200 483 -202 51 -1 94 20 116 57 18 30 19 50 16 267 -1 129 -3 236 -4 236 -1 1 -38 -4 -82 -12 -263 -43 -497 16 -646 165 -58 58 -76 86 -134 206 -94 196 -149 271 -255 348 -78 56 -120 98 -120 119 0 50 121 67 217 30 125 -47 208 -119 299 -258 126 -191 284 -276 485 -261 77 6 233 49 245 68 3 6 13 38 20 71 19 80 68 181 110 228 40 45 44 42 -86 60 -103 14 -282 60 -368 94 -362 143 -571 410 -649 829 -25 134 -25 463 0 567 36 149 103 285 195 398 l43 53 -15 47 c-52 161 -46 370 16 564 20 61 24 67 54 73 107 20 352 -68 576 -206 l98 -61 87 20 c204 47 298 57 553 57 256 0 349 -10 553 -57 l88 -20 77 48 c153 96 304 167 407 193 85 21 159 31 191 25 28 -5 33 -12 52 -72 57 -178 67 -404 23 -542 l-21 -65 20 -25 c98 -121 175 -269 212 -409 26 -98 36 -348 19 -488 -74 -608 -407 -935 -1041 -1023 -120 -16 -118 -15 -83 -53 46 -48 90 -134 115 -223 20 -74 22 -106 27 -529 7 -492 5 -482 66 -507 47 -20 102 -13 200 26 348 135 622 311 884 569 417 410 679 942 752 1527 20 159 18 498 -5 655 -173 1219 -1151 2131 -2373 2215 -161 11 -175 11 -339 0z"></path></g></svg></a><a class="button button--link" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fjiangmiemie.com%2Fdocs%2F%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259A%2584%25E5%25BA%2594%25E7%2594%25A8%2F%25E6%25A8%25A1%25E5%259E%258B%25E7%25A4%25BE%25E5%258C%25BA&amp;text=%E6%88%91%E5%88%9A%E5%88%9A%E8%AF%BB%E4%BA%86%20%22%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%22%20by%20%40jiangyangcreate" target="_blank" rel="noreferrer noopener" aria-label="share on twitter" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 24 24" height="20" width="20" style="margin-right:7px"><g fill="url(#grad1)"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></g></svg></a><a class="button button--link" href="https://jiangmiemie.com/blog/rss.xml" target="_blank" rel="noreferrer noopener" aria-label="rss reader link" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 16 16" height="20" width="20" style="margin-right:7px"><g fill="url(#grad1)"><linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#D8262C;stop-opacity:1"></stop><stop offset="100%" style="stop-color:#E6B800;stop-opacity:1"></stop></linearGradient><circle cx="3" cy="13" r="2"></circle><path d="M1 5.667v2.667A6.674 6.674 0 0 1 7.667 15h2.666c0-5.146-4.187-9.333-9.333-9.333z"></path><path d="M1 1v2.667C7.25 3.667 12.334 8.75 12.334 15H15C15 7.28 8.72 1 1 1z"></path></g></svg></a><a class="button button--link" href="mailto:jiangyangcreate@gmail.com" target="_blank" rel="noreferrer noopener" aria-label="send email" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 1024 1024" style="margin-right:7px;height:20px;width:20px"><g><path style="opacity:0.996;fill:url(#grad1)" d="M 243.5,420.5 C 189.523,379.185 135.857,337.518 82.5,295.5C 140.987,183.298 229.487,104.632 348,59.5C 432.42,30.213 518.587,23.713 606.5,40C 693.576,57.6997 769.243,96.6997 833.5,157C 787.833,202.667 742.167,248.333 696.5,294C 695.5,294.667 694.5,294.667 693.5,294C 642.378,246.775 581.711,222.942 511.5,222.5C 407.283,227.337 327.117,273.337 271,360.5C 259.938,379.624 250.771,399.624 243.5,420.5 Z"></path></g><g><path style="opacity:0.996;fill:url(#grad1)" d="M 82.5,295.5 C 135.857,337.518 189.523,379.185 243.5,420.5C 223.5,481.168 223.5,541.835 243.5,602.5C 189.523,643.815 135.857,685.482 82.5,727.5C 31.7513,624.612 18.5846,516.945 43,404.5C 51.5465,366.35 64.7132,330.016 82.5,295.5 Z"></path></g><g><path style="opacity:0.998;fill:url(#grad1)" d="M 829.5,876.5 C 777.833,835.833 726.167,795.167 674.5,754.5C 725.318,718.61 756.985,669.944 769.5,608.5C 683.503,607.5 597.503,607.167 511.5,607.5C 511.5,546.167 511.5,484.833 511.5,423.5C 662.5,423.5 813.5,423.5 964.5,423.5C 979.017,509.532 974.517,594.532 951,678.5C 928.206,755.703 887.706,821.703 829.5,876.5 Z"></path></g><g><path style="opacity:0.997;fill:url(#grad1)" d="M 243.5,602.5 C 274.71,690.744 334.043,751.577 421.5,785C 487.78,805.861 553.78,804.861 619.5,782C 638.723,774.388 657.056,765.222 674.5,754.5C 726.167,795.167 777.833,835.833 829.5,876.5C 762.129,936.339 683.462,972.839 593.5,986C 437.233,1008.45 300.566,967.451 183.5,863C 141.888,823.583 108.222,778.416 82.5,727.5C 135.857,685.482 189.523,643.815 243.5,602.5 Z"></path></g></svg></a></div></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/大模型的应用/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">大模型的应用</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/大模型的应用/上下文工程/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">上下文工程</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#开源模型" class="table-of-contents__link toc-highlight">开源模型</a><ul><li><a href="#模型架构" class="table-of-contents__link toc-highlight">模型架构</a></li><li><a href="#参数权重" class="table-of-contents__link toc-highlight">参数权重</a></li><li><a href="#演化过程" class="table-of-contents__link toc-highlight">演化过程</a></li></ul></li><li><a href="#大模型社区" class="table-of-contents__link toc-highlight">大模型社区</a><ul><li><a href="#开源模型推理" class="table-of-contents__link toc-highlight">开源模型推理</a></li><li><a href="#开源数据集" class="table-of-contents__link toc-highlight">开源数据集</a></li></ul></li><li><a href="#模型评测" class="table-of-contents__link toc-highlight">模型评测</a><ul><li><a href="#自动化评测" class="table-of-contents__link toc-highlight">自动化评测</a></li><li><a href="#多模态自动化评测" class="table-of-contents__link toc-highlight">多模态自动化评测</a></li></ul></li><li><a href="#微调与量化" class="table-of-contents__link toc-highlight">微调与量化</a><ul><li><a href="#lora-微调" class="table-of-contents__link toc-highlight">LoRA 微调</a></li><li><a href="#冻结层微调" class="table-of-contents__link toc-highlight">冻结层微调</a></li><li><a href="#全量微调" class="table-of-contents__link toc-highlight">全量微调</a></li><li><a href="#量化" class="table-of-contents__link toc-highlight">量化</a></li></ul></li><li><a href="#模型本地部署" class="table-of-contents__link toc-highlight">模型本地部署</a><ul><li><a href="#ollama示例" class="table-of-contents__link toc-highlight">ollama示例</a></li><li><a href="#vllm示例" class="table-of-contents__link toc-highlight">vllm示例</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright"><div style="font-size: 0.75rem;">Copyright ©  jiangyang 2026<div></div></div></div></div></footer></div>
</body>
</html>