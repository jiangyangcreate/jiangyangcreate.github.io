---
sidebar_position: 6
title: 模型微调
---

模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。
:::info
把模型想象为一个固定容量的大脑，脑容量是固定的，通过微调让它对A印象深刻，它就会淡忘B。
:::

LLaMA-Factory 是基于 LLaMA 的模型微调框架，支持 LoRA 微调、冻结层微调、全量微调。既可以通过 WebUI 微调，也可以通过命令行微调。

项目地址：[https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

安装

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install --no-deps -e .
```

启动 WebUI

```bash
llamafactory-cli webui
```

在操作界面我们会看到许多可选项，下面是一些常用选项的说明。

## 微调

微调有三种方法：LoRA 微调、冻结层微调、全量微调。

以微调一个 **14B 参数**的大型语言模型为例，以下是对三种常见微调方法的硬件需求和大致时间的估算：

### LoRA 微调

**硬件需求**：

- **显存**：~24GB 显存以上即可（如 NVIDIA RTX 3090 或 A100）。LoRA 主要通过冻结大部分模型参数，仅在少量层中插入低秩适配模块，因此显存需求较低。
- **GPU 数量**：单卡或 2 卡即可应付中等规模的微调任务。

**时间估算**：

- **小规模数据**（如数十万条数据，100k steps 以内）：几个小时到 1 天。
- **大规模数据**（如百万条数据，300k steps）：2-3 天。

**优点**：

- 高效，显存需求低，适合个人开发者或中小型实验室。
- 微调后的模型参数（LoRA 插件）仅几百 MB，方便存储和共享。

### 冻结层微调（Freeze）

**硬件需求**：

- **显存**：需要 48GB 以上显存（如 A6000 或 80GB A100）。冻结大部分模型参数，微调后几层或部分特定模块。
- **GPU 数量**：1-4 张高性能 GPU。

**时间估算**：

- **小规模数据**：1-2 天。
- **大规模数据**：3-5 天。

**优点**：

- 显存需求较低，性能适中。
- 微调时间比全量微调短，但仍需较多资源。

### 全量微调（Full Fine-tuning）

**硬件需求**：

- **显存**：需 80GB 显存以上的高性能 GPU（如 NVIDIA A100 或 H100）。14B 参数模型通常需要 4 卡或更多 GPU 的分布式训练。
- **GPU 数量**：4-8 张 A100（或等效）GPU。对于超大模型，可能需要更多。

**时间估算**：

- **小规模数据**：2-3 天。
- **大规模数据**：5 天到 1 周甚至更长。

**优点**：

- 微调灵活性最高，可以针对特定任务完全优化模型。
- 模型质量可能略高于 LoRA 和冻结层微调。

**缺点**：

- 显存需求高，成本昂贵。
- 微调后模型体积巨大，通常需要数百 GB 存储。

有些框架可以以时间换空间，例如使用梯度检查点：在显存不足时保存中间计算结果，降低瞬时显存需求。或者以精度换空间，例如使用混合精度训练：可以显著减少显存需求和训练时间。有兴趣可以自行搜索。

## 量化

随着语言模型规模的不断增大，其训练的难度和成本已成为共识。 而随着用户数量的增加，模型推理的成本也在不断攀升，甚至可能成为限制模型部署的首要因素。 因此，我们需要对模型进行压缩以加速推理过程，而模型量化是其中一种有效的方法。

大语言模型的参数通常以高精度浮点数存储，这导致模型推理需要大量计算资源。 量化技术通过将高精度数据类型存储的参数转换为低精度数据类型存储， 可以在不改变模型参数量和架构的前提下加速推理过程。这种方法使得模型的部署更加经济高效，也更具可行性。

浮点数一般由 3 部分组成：符号位、指数位和尾数位。指数位越大，可表示的数字范围越大。尾数位越大、数字的精度越高。

量化可以根据何时量化分为：后训练量化和训练感知量化，也可以根据量化参数的确定方式分为：静态量化和动态量化。

### QAT: 感知量化训练
在训练感知量化（QAT, **Quantization-Aware Training**）中，模型一般在预训练过程中被量化，然后又在训练数据上再次微调，得到最后的量化模型。

---

### PTQ: 后训练量化
后训练量化（PTQ, **Post-Training Quantization**）一般指在模型预训练完成后，基于校准数据集（**calibration dataset**）确定量化参数进而对模型进行量化。

---

### GPTQ: 分组精度调优量化
GPTQ（**Group-wise Precision Tuning Quantization**）是一种静态的后训练量化技术。  
“静态”指的是预训练模型一旦确定，经过量化后量化参数不再更改。  
**GPTQ 的特点**：
- 将 fp16 精度模型量化为 4-bit
- 节省约 75% 的显存
- 大幅提高推理速度  

**使用示例**：
```yaml
model_name_or_path: TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ
```

---

### AWQ: 激活感知层量化
AWQ（**Activation-Aware Layer Quantization**）是一种静态的后训练量化技术。其核心思想为：少部分重要权重保持不被量化，以维持模型性能。  
**AWQ 的优势**：
- 所需校准数据集更小
- 在指令微调和多模态模型上表现良好  

**使用示例**：
```yaml
model_name_or_path: TechxGenus/Meta-Llama-3-8B-Instruct-AWQ
```

---

### AQLM: 语言模型加法量化
AQLM（**Additive Quantization of Language Models**）是一种仅对模型权重进行量化的 PTQ 方法。  
**特点**：
- 在 2-bit 量化下达到了最佳性能
- 在 3-bit 和 4-bit 量化下同样表现优异
- 2-bit 量化适用于低显存部署大模型  

尽管 AQLM 的推理速度提升并不显著，但其极低的显存占用具有很高实用价值。

---

### OFTQ: 动态后训练量化
OFTQ（**On-the-fly Quantization**）无需校准数据集，直接在推理阶段进行动态量化。  
**特点**：
- 无需校准数据集
- 能够在推理阶段动态量化，保持性能  

**使用示例**：
```yaml
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
quantization_bit: 4
quantization_method: bitsandbytes  # 可选: [bitsandbytes (4/8), hqq (2/3/4/5/6/8), eetq (8)]
```

---

### bitsandbytes: 动态后训练量化
区别于 GPTQ，**bitsandbytes** 是一种动态的后训练量化技术。  
**特点**：
- 支持大于 1B 的模型量化
- 在 8-bit 量化后，性能损失极小
- 能够节省约 50% 的显存  

---

### HQQ: 半二次量化
HQQ（**Half-Quadratic Quantization**）在准确度和速度之间取得平衡。  
**特点**：
- 不需要校准阶段
- 推理速度极快
- 准确度与需要校准数据的方法相当  

HQQ 是一种动态的后训练量化方法，适合需要快速推理且性能敏感的场景。

---

### EETQ: 高效变换器量化
EETQ（**Easy and Efficient Quantization for Transformers**）是一种只对模型权重进行量化的 PTQ 方法。  
**特点**：
- 速度快
- 简单易用  

EETQ 特别适合对性能和实现复杂度都有较高要求的用户。
