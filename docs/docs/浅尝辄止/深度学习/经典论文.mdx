---
sidebar_position: 99
title: ç»å…¸è®ºæ–‡ğŸ”¨
---

## LeNet-5

LeNet-5è¢«è®¤ä¸ºæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å¼€å±±ä¹‹ä½œï¼Œä¸»è¦ç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚å®ƒåŒ…å«äº†å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ï¼Œæ˜¯ç°ä»£CNNçš„é›å½¢ã€‚

### å·ç§¯

å·ç§¯ç¥ç»ç½‘ç»œ(convolutional neural network)ç®€ç§°CNNã€‚å·ç§¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯å·ç§¯æ ¸ï¼Œå·ç§¯æ ¸åœ¨å›¾åƒå¤„ç†é¢†åŸŸå¯ä»¥ç”¨æ¥æå–å›¾åƒçš„çºµå‘å’Œæ¨ªå‘ç‰¹å¾ã€‚

å·ç§¯æ ¸çš„å¤§å°ä¸€èˆ¬ä¸ºå¥‡æ•°ï¼Œå¦‚3x3ï¼Œ5x5ï¼Œ7x7ç­‰ï¼Œå·ç§¯æ ¸é€šå¸¸ä¸å›¾åƒå¤„ç†ï¼ˆover paddingï¼‰åçš„å›¾åƒè¿›è¡Œå·ç§¯æ“ä½œï¼Œå·ç§¯æ ¸åœ¨å›¾åƒä¸Šæ»‘åŠ¨ï¼Œæ¯æ¬¡æ»‘åŠ¨ä¸€ä¸ªåƒç´ ï¼Œå¯¹åº”ä½ç½®çš„åƒç´ å€¼ä¸å·ç§¯æ ¸å¯¹åº”ä½ç½®çš„å€¼ç›¸ä¹˜ï¼Œç„¶åæ±‚å’Œï¼Œæœ€åå°†æ±‚å’Œçš„ç»“æœä½œä¸ºå·ç§¯æ ¸ä¸­å¿ƒåƒç´ çš„å€¼ï¼Œè¿™æ ·å°±å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„å›¾åƒã€‚

æ–°çš„å›¾åƒå¯ä»¥ç”¨æ›´å°‘çš„æ•°æ®ååº”å‡ºå›¾åƒçš„ç‰¹å¾ã€‚è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ç‰¹å¾æå–ã€‚

æˆ‘ä»¬ä»ä¸€ä¸ª6x6çš„çŸ©é˜µå¼€å§‹ï¼š

$$
\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46} \\
a_{51} & a_{52} & a_{53} & a_{54} & a_{55} & a_{56} \\
a_{61} & a_{62} & a_{63} & a_{64} & a_{65} & a_{66} \\
\end{bmatrix}
$$



æˆ‘ä»¬çš„å·ç§¯æ ¸æ˜¯ä¸€ä¸ª3x3çš„çŸ©é˜µï¼š

$$
\mathbf{K} = \begin{bmatrix}
k_{11} & k_{12} & k_{13} \\
k_{21} & k_{22} & k_{23} \\
k_{31} & k_{32} & k_{33} \\
\end{bmatrix}
$$


æˆ‘ä»¬å‡è®¾å·ç§¯æ ¸ä½äºåŸå§‹çŸ©é˜µçš„å·¦ä¸Šè§’ï¼Œè¦†ç›–çš„åŒºåŸŸå¦‚ä¸‹ï¼š

$$
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{bmatrix}
$$

æ­¤æ—¶ï¼Œè¾“å‡ºçŸ©é˜µçš„ç¬¬ä¸€ä¸ªå…ƒç´ $O_{11}$çš„è®¡ç®—ä¸ºï¼š

$$
\begin{aligned}
O_{11} &= k_{11} \cdot a_{11} + k_{12} \cdot a_{12} + k_{13} \cdot a_{13} \\ 
&\quad + k_{21} \cdot a_{21} + k_{22} \cdot a_{22} + k_{23} \cdot a_{23} \\ 
&\quad + k_{31} \cdot a_{31} + k_{32} \cdot a_{32} + k_{33} \cdot a_{33}
\end{aligned}
$$

æ•´ä¸ªè¾“å‡ºçŸ©é˜µ

å·ç§¯æ ¸åœ¨æ•´ä¸ª6x6çŸ©é˜µä¸Šæ»‘åŠ¨(ä»å·¦è‡³å³ï¼Œä»ä¸Šè‡³ä¸‹)ï¼Œç”Ÿæˆä¸€ä¸ª4x4çš„è¾“å‡ºçŸ©é˜µã€‚è¾“å‡ºçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ éƒ½æŒ‰ç…§ä¸Šè¿°æ–¹å¼è®¡ç®—ã€‚

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹å·ç§¯æ ¸åŠ¨ç”»</summary>
``` jsx live
// ä½ å¯ä»¥å°è¯•æ›´æ”¹çŸ©é˜µå°ºå¯¸ä¸å·ç§¯æ ¸çš„å°ºå¯¸æ¥æ„Ÿå—å·ç§¯è¿‡ç¨‹
function example(props) {
  // ä½¿ç”¨ XPath æŸ¥è¯¢é€‰æ‹©è¾“å‡ºæ¡†
  const xpathSelector =
    "/html/body/div/div[2]/div/div/main/div/div/div/div/article/div[2]/div[1]/div[4]";
  const myElement = document.evaluate(
    xpathSelector,
    document,
    null,
    XPathResult.FIRST_ORDERED_NODE_TYPE,
    null
  ).singleNodeValue;
  // çŸ©é˜µå°ºå¯¸
  const matrixSize = 6;
  // å·ç§¯æ ¸å°ºå¯¸
  const kernelSize = 3;
  const matrix = Array.from({ length: matrixSize }, (_, i) =>
    Array.from({ length: matrixSize }, (_, j) => `a${i + 1}${j + 1}`)
  );
  const [position, setPosition] = useState([0, 0]);
  useEffect(() => {
    const positions = [];
    for (let i = 0; i <= matrixSize - kernelSize; i++) {
      for (let j = 0; j <= matrixSize - kernelSize; j++) {
        positions.push([i, j]);
      }
    }

    let index = 0;
    const interval = setInterval(() => {
      setPosition(positions[index]);
      index = (index + 1) % positions.length;
    }, 1000);

    return () => clearInterval(interval);
  }, []);

  return (
    <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh', backgroundColor: '#f0f0f0' }}>
      <div style={{ display: 'grid', gridTemplateColumns: `repeat(${matrixSize}, 50px)`, gridGap: '5px', position: 'relative' }}>
        {matrix.map((row, i) =>
          row.map((cell, j) => (
            <div
              key={`${i}-${j}`}
              style={{
                width: '50px',
                height: '50px',
                backgroundColor: '#fff',
                border: '1px solid #ccc',
                display: 'flex',
                justifyContent: 'center',
                alignItems: 'center',
                fontSize: '18px',
                backgroundColor: i >= position[0] && i < position[0] + kernelSize && j >= position[1] && j < position[1] + kernelSize ? 'yellow' : '#fff'
              }}
            >
              {cell}
            </div>
          ))
        )}
      </div>
    </div>
  );
}
```
</details>


æœ€ç»ˆè¾“å‡ºçŸ©é˜µ$\mathbf{O}$ä¸ºï¼š

$$
\mathbf{O} = \begin{bmatrix}
O_{11} & O_{12} & O_{13} & O_{14} \\
O_{21} & O_{22} & O_{23} & O_{24} \\
O_{31} & O_{32} & O_{33} & O_{34} \\
O_{41} & O_{42} & O_{43} & O_{44} \\
\end{bmatrix}
$$

æ¯ä¸ª$O_{ij}$çš„å…·ä½“è®¡ç®—æ–¹æ³•å¦‚å‰æ‰€è¿°ï¼Œé€šè¿‡å·ç§¯æ ¸åœ¨åŸå§‹çŸ©é˜µä¸Šçš„æ»‘åŠ¨å’Œè®¡ç®—å¾—åˆ°ã€‚

é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œå¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°å·ç§¯æ ¸æ˜¯å¦‚ä½•å¯¹çŸ©é˜µè¿›è¡Œæ“ä½œå¹¶ç”Ÿæˆè¾“å‡ºçš„ã€‚

#### å¸¸è§å·ç§¯æ ¸


1. **æ°´å¹³è¾¹ç¼˜æ£€æµ‹**ï¼š
   $$
   \begin{bmatrix}
   -1 & -1 & -1 \\
   0 & 0 & 0 \\
   1 & 1 & 1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ£€æµ‹æ°´å¹³è¾¹ç¼˜ã€‚

2. **å‚ç›´è¾¹ç¼˜æ£€æµ‹**ï¼š
   $$
   \begin{bmatrix}
   -1 & 0 & 1 \\
   -1 & 0 & 1 \\
   -1 & 0 & 1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ£€æµ‹å‚ç›´è¾¹ç¼˜ã€‚

3. **Sobelç®—å­ï¼ˆæ°´å¹³ï¼‰**ï¼š
   $$
   \begin{bmatrix}
   -1 & 0 & 1 \\
   -2 & 0 & 2 \\
   -1 & 0 & 1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ£€æµ‹æ°´å¹³è¾¹ç¼˜å’Œæ¢¯åº¦ã€‚

4. **Sobelç®—å­ï¼ˆå‚ç›´ï¼‰**ï¼š
   $$
   \begin{bmatrix}
   1 & 2 & 1 \\
   0 & 0 & 0 \\
   -1 & -2 & -1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ£€æµ‹å‚ç›´è¾¹ç¼˜å’Œæ¢¯åº¦ã€‚

5. **æ‹‰æ™®æ‹‰æ–¯ç®—å­**ï¼š
   $$
   \begin{bmatrix}
   0 & 1 & 0 \\
   1 & -4 & 1 \\
   0 & 1 & 0
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ£€æµ‹å›¾åƒçš„äºŒé˜¶å¯¼æ•°ï¼Œå¼ºè°ƒè¾¹ç¼˜ã€‚

6. **é”åŒ–**ï¼š
   $$
   \begin{bmatrix}
   0 & -1 & 0 \\
   -1 & 5 & -1 \\
   0 & -1 & 0
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæé«˜å›¾åƒçš„æ¸…æ™°åº¦ã€‚

7. **é«˜æ–¯æ¨¡ç³Šï¼ˆ3x3ï¼‰**ï¼š
   $$
   \frac{1}{16}
   \begin{bmatrix}
   1 & 2 & 1 \\
   2 & 4 & 2 \\
   1 & 2 & 1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šå¹³æ»‘å›¾åƒï¼Œå‡å°‘å™ªå£°ã€‚

8. **é«˜æ–¯æ¨¡ç³Šï¼ˆ5x5ï¼‰**ï¼š
   $$
   \frac{1}{256}
   \begin{bmatrix}
   1 & 4 & 6 & 4 & 1 \\
   4 & 16 & 24 & 16 & 4 \\
   6 & 24 & 36 & 24 & 6 \\
   4 & 16 & 24 & 16 & 4 \\
   1 & 4 & 6 & 4 & 1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šæ›´å¼ºçš„å¹³æ»‘æ•ˆæœã€‚

9. **è¾¹ç¼˜å¢å¼º**ï¼š
   $$
   \begin{bmatrix}
   -1 & -1 & -1 \\
   -1 & 9 & -1 \\
   -1 & -1 & -1
   \end{bmatrix}
   $$
   ç”¨é€”ï¼šå¢å¼ºè¾¹ç¼˜ï¼Œä½¿å›¾åƒè½®å»“æ›´åŠ æ˜æ˜¾ã€‚

10. **å‡å€¼æ»¤æ³¢**ï¼š
    $$
    \frac{1}{9}
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{bmatrix}
    $$
    ç”¨é€”ï¼šå‡åŒ€åœ°å¹³æ»‘å›¾åƒã€‚

```python showLineNumbers
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
import cv2

# è®¾ç½®ä¸­æ–‡å­—ä½“
# æ›¿æ¢ä¸ºä½ ç³»ç»Ÿä¸­æ”¯æŒä¸­æ–‡çš„å­—ä½“è·¯å¾„(windows)
font_path = r'C:\Windows\Fonts\simhei.ttf'  
# macï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
# font_path = '/System/Library/Fonts/STHeiti Light.ttc' 
font_prop = FontProperties(fname=font_path)

# è¯»å–ç°åº¦å›¾åƒ
image = np.array(cv2.imread('data/people.bmp',cv2.IMREAD_GRAYSCALE))

# å®šä¹‰å·ç§¯æ ¸
kernels = {
    'æ°´å¹³è¾¹ç¼˜': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),
    'å‚ç›´è¾¹ç¼˜': np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),
    'Sobelæ°´å¹³': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),
    'Sobelå‚ç›´': np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]),
    'æ‹‰æ™®æ‹‰æ–¯': np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]),
    'é”åŒ–': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),
    'é«˜æ–¯æ¨¡ç³Š3x3': np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16,
    'é«˜æ–¯æ¨¡ç³Š5x5': np.array([[1, 4, 6, 4, 1], [4, 16, 24, 16, 4], [6, 24, 36, 24, 6], [4, 16, 24, 16, 4], [1, 4, 6, 4, 1]]) / 256,
    'è¾¹ç¼˜å¢å¼º': np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]),
    'å‡å€¼æ»¤æ³¢': np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) / 9
}

# ä½¿ç”¨NumPyå®ç°å·ç§¯æ“ä½œ
def convolve2d(image, kernel):
    # è·å–å›¾åƒå’Œå·ç§¯æ ¸çš„å°ºå¯¸
    i_height, i_width = image.shape
    k_height, k_width = kernel.shape
    
    # è®¡ç®—è¾“å‡ºå›¾åƒçš„å°ºå¯¸
    o_height = i_height - k_height + 1
    o_width = i_width - k_width + 1
    
    # åˆ›å»ºè¾“å‡ºå›¾åƒ
    output = np.zeros((o_height, o_width))
    
    # æ‰§è¡Œå·ç§¯æ“ä½œ
    for y in range(o_height):
        for x in range(o_width):
            # æå–å›¾åƒåŒºåŸŸ
            region = image[y:y+k_height, x:x+k_width]
            # è®¡ç®—å·ç§¯å€¼
            output[y, x] = np.sum(region * kernel)
    
    return output

# åº”ç”¨å·ç§¯æ ¸
results = {}
for name, kernel in kernels.items():
    # ä¸ºäº†å¤„ç†è¾¹ç•Œï¼Œå…ˆå¯¹å›¾åƒè¿›è¡Œå¡«å……
    if kernel.shape[0] == 5:  # å¯¹äº5x5å·ç§¯æ ¸
        pad_width = 2
    else:  # å¯¹äº3x3å·ç§¯æ ¸
        pad_width = 1
    
    padded_image = np.pad(image, pad_width, mode='constant')
    filtered_image = convolve2d(padded_image, kernel)
    
    # å½’ä¸€åŒ–å¤„ç†ï¼Œç¡®ä¿åƒç´ å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
    filtered_image = np.clip(filtered_image, 0, 255).astype(np.uint8)
    results[name] = filtered_image

# æ˜¾ç¤ºç»“æœ
plt.figure(figsize=(15, 8))
for i, (name, result) in enumerate(results.items()):
    plt.subplot(3, 4, i + 1)
    plt.imshow(result, cmap='gray')
    plt.title(name, fontproperties=font_prop)
    plt.axis('off')

plt.tight_layout()
plt.show()

```

### æ± åŒ–


æ± åŒ–ï¼ˆPoolingï¼‰æ˜¯ä¸€ç§ç”¨äºå‡å°‘å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­ç‰¹å¾å›¾å¤§å°çš„æ“ä½œã€‚å®ƒé€šè¿‡å°†ç‰¹å¾å›¾ä¸Šçš„å±€éƒ¨åŒºåŸŸè¿›è¡Œèšåˆï¼Œå¾—åˆ°ä¸€ä¸ªæ›´å°çš„ç‰¹å¾å›¾ã€‚

æ± åŒ–æ“ä½œç±»ä¼¼å·ç§¯æ“ä½œï¼Œä½¿ç”¨çš„ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå°çš„çŸ©é˜µï¼Œå«åšæ± åŒ–æ ¸ï¼Œä½†æ˜¯æ± åŒ–æ ¸æœ¬èº«æ²¡æœ‰å‚æ•°ï¼Œåªæ˜¯é€šè¿‡å¯¹è¾“å…¥ç‰¹å¾çŸ©é˜µæœ¬èº«è¿›è¡Œè¿ç®—ï¼Œå®ƒçš„å¤§å°é€šå¸¸æ˜¯2x2ã€3x3ã€4x4ç­‰ï¼Œç„¶åå°†æ± åŒ–æ ¸åœ¨å·ç§¯å¾—åˆ°çš„è¾“å‡ºç‰¹å¾å›¾ä¸­è¿›è¡Œæ± åŒ–æ“ä½œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ± åŒ–çš„è¿‡ç¨‹ä¸­ä¹Ÿæœ‰Paddingæ–¹å¼ä»¥åŠæ­¥é•¿çš„æ¦‚å¿µï¼Œä¸å·ç§¯ä¸åŒçš„æ˜¯ï¼Œæ± åŒ–çš„æ­¥é•¿å¾€å¾€ç­‰äºæ± åŒ–æ ¸çš„å¤§å°ã€‚
æœ€å¸¸è§çš„æ± åŒ–æ“ä½œä¸º**æœ€å¤§å€¼æ± åŒ–**ï¼ˆMax Poolingï¼‰å’Œ**å¹³å‡å€¼æ± åŒ–**ï¼ˆAverage Poolingï¼‰ä¸¤ç§ã€‚
```python showLineNumbers
import numpy as np

def pooling(input_array, pool_size=(2, 2), stride=None, mode='max'):
    """
    æ± åŒ–æ“ä½œå‡½æ•°
    
    å‚æ•°:
        input_array: è¾“å…¥æ•°ç»„ï¼Œå½¢çŠ¶ä¸º[height, width]æˆ–[batch, height, width, channels]
        pool_size: æ± åŒ–çª—å£å¤§å°ï¼Œé»˜è®¤ä¸º(2, 2)
        stride: æ­¥é•¿ï¼Œé»˜è®¤ä¸pool_sizeç›¸åŒ
        mode: æ± åŒ–ç±»å‹ï¼Œ'max'è¡¨ç¤ºæœ€å¤§æ± åŒ–ï¼Œ'avg'è¡¨ç¤ºå¹³å‡æ± åŒ–
        
    è¿”å›:
        æ± åŒ–åçš„æ•°ç»„
    """
    # å¦‚æœæœªæŒ‡å®šstrideï¼Œåˆ™é»˜è®¤ä¸pool_sizeç›¸åŒ
    if stride is None:
        stride = pool_size
    
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    input_array = np.asarray(input_array)
    
    # å¤„ç†ä¸åŒç»´åº¦çš„è¾“å…¥
    if input_array.ndim == 2:  # å•é€šé“2Dè¾“å…¥
        h, w = input_array.shape
        d = 1
        input_array = input_array.reshape(1, h, w, 1)
    elif input_array.ndim == 3:  # å¸¦æ‰¹æ¬¡æˆ–é€šé“çš„3Dè¾“å…¥
        raise ValueError("è¾“å…¥æ•°ç»„ç»´åº¦åº”ä¸º2Dæˆ–4D")
    elif input_array.ndim == 4:  # æ ‡å‡†4Dè¾“å…¥ [batch, height, width, channels]
        pass
    else:
        raise ValueError("è¾“å…¥æ•°ç»„ç»´åº¦åº”ä¸º2Dæˆ–4D")
    
    # è·å–è¾“å…¥å°ºå¯¸
    batch_size, height, width, channels = input_array.shape
    
    # è®¡ç®—è¾“å‡ºå°ºå¯¸
    out_height = (height - pool_size[0]) // stride[0] + 1
    out_width = (width - pool_size[1]) // stride[1] + 1
    
    # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„
    output = np.zeros((batch_size, out_height, out_width, channels))
    
    # æ‰§è¡Œæ± åŒ–æ“ä½œ
    for b in range(batch_size):
        for c in range(channels):
            for i in range(out_height):
                for j in range(out_width):
                    h_start = i * stride[0]
                    h_end = h_start + pool_size[0]
                    w_start = j * stride[1]
                    w_end = w_start + pool_size[1]
                    
                    pool_region = input_array[b, h_start:h_end, w_start:w_end, c]
                    
                    if mode == 'max':
                        output[b, i, j, c] = np.max(pool_region)
                    elif mode == 'avg':
                        output[b, i, j, c] = np.mean(pool_region)
                    else:
                        raise ValueError("æ”¯æŒçš„æ¨¡å¼ä¸º'max'æˆ–'avg'")
    
    # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œåˆ™è¿”å›2Dè¾“å‡º
    if input_array.shape[0] == 1 and input_array.shape[3] == 1:
        return output[0, :, :, 0]
    
    return output

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.array([
        [1, 2, 3, 4],
        [5, 6, 7, 8],
        [9, 10, 11, 12],
        [13, 14, 15, 16]
    ])
    
    # æœ€å¤§æ± åŒ–
    max_pooled = pooling(test_data, pool_size=(2, 2), mode='max')
    print("æœ€å¤§æ± åŒ–ç»“æœ:")
    print(max_pooled)
    
    # å¹³å‡æ± åŒ–
    avg_pooled = pooling(test_data, pool_size=(2, 2), mode='avg')
    print("å¹³å‡æ± åŒ–ç»“æœ:")
    print(avg_pooled)
'''
æœ€å¤§æ± åŒ–ç»“æœ:
[[ 6.  8.]
 [14. 16.]]
å¹³å‡æ± åŒ–ç»“æœ:
[[ 3.5  5.5]
 [11.5 13.5]]
'''
```

- over padding(å¡«å……)
  
æœ‰æ—¶å›¾åƒçš„ç‰¹å¾åœ¨è¾¹ç¼˜ä¸Šï¼Œä¾‹å¦‚

```python
import numpy as np
import matplotlib.pyplot as plt

# è¯»å–å›¾åƒ
inputs = np.array([
  [255,1,2],
  [255,1,2],
  [255,1,2],]

)
# ç”¨äºæå–çºµå‘ç‰¹å¾çš„å·ç§¯æ ¸
kernel = np.array([
  [0,1,0],
  [0,1,0],
  [0,1,0]]
)

# å·ç§¯æ“ä½œç»“æœï¼Œæ²¡èƒ½æ­£ç¡®è·å–è¾¹ç¼˜çš„ç‰¹å¾
'''
[[0.   2.   0.]
 [0.   2.   0.]
 [0.   2.   0.]]
'''


# å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå¡«å……
# array: éœ€è¦å¡«å……çš„æ•°ç»„
# pad_width: å¡«å……çš„å®½åº¦(ä¸Šä¸‹å·¦å³éƒ½å¡«å……)
# mode: å¡«å……çš„æ–¹å¼ï¼Œé€šå¸¸ä¸º'constant',
# æœ‰0ã€ç©ºã€æœ€å¤§ã€å¹³å‡ã€ä¸­ä½ç­‰11ç§å‚æ•°å¯ä»¥é€‰ï¼Œç‚¹å‡»æ–¹æ³•è¿›å…¥æŸ¥çœ‹
# constant_values: å¡«å……çš„å€¼ï¼Œé€šå¸¸ä¸º0
inputs = np.pad(
    array=inputs,
    pad_width=1,
    mode='constant',
    constant_values=0
)


# å·ç§¯æ“ä½œ
out_put = np.zeros((inputs.shape[0] - kernel.shape[0] + 1, inputs.shape[1] - kernel.shape[1] + 1))
out_put_w = out_put.shape[0]
out_put_h = out_put.shape[1]

for i in range(out_put_w):
    for j in range(out_put_h):
        conv_result = np.sum(inputs[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)
        out_put[i][j] = conv_result

# å·ç§¯æ“ä½œç»“æœ,æ­£ç¡®çš„è·å–åˆ°äº†è¾¹ç¼˜çš„ç‰¹å¾
print(out_put)
'''
[[510.   2.   4.]
 [765.   3.   6.]
 [510.   2.   4.]]
'''
```


### æ­¥å¹…

æ­¥å¹…è¡¨ç¤ºå·ç§¯æ ¸ç§»åŠ¨çš„æ­¥é•¿ï¼Œæ­¥å¹…è¶Šå¤§ï¼Œå·ç§¯æ ¸æ¯æ¬¡è·³è·ƒçš„è·ç¦»å°±è¶Šå¤šï¼Œå·ç§¯æ ¸çš„æ„Ÿå—é‡è¶Šå°ã€‚
:::tip
æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰çš„å®šä¹‰ï¼šæºè‡ªç”Ÿç‰©ä¸“ä¸šæœ¯è¯­ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­è¡¨ç¤ºå·ç§¯ç¥ç»ç½‘ç»œæ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ï¼ˆfeature mapï¼‰ä¸Šçš„åƒç´ ç‚¹æ˜ å°„å›è¾“å…¥å›¾åƒä¸Šçš„åŒºåŸŸå¤§å°ã€‚é€šä¿—ç‚¹çš„è§£é‡Šæ˜¯ï¼Œç‰¹å¾å›¾ä¸Šä¸€ç‚¹ï¼Œç›¸å¯¹äºåŸå›¾çš„å¤§å°ï¼Œä¹Ÿæ˜¯å·ç§¯ç¥ç»ç½‘ç»œç‰¹å¾æ‰€èƒ½çœ‹åˆ°è¾“å…¥å›¾åƒçš„åŒºåŸŸã€‚
:::

```python showLineNumbers
import numpy as np

def convolution_2d(input_array, kernel, stride=3):
    """
    å®ç°2Då·ç§¯æ“ä½œ
    
    å‚æ•°:
        input_array: è¾“å…¥æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (height, width)
        kernel: å·ç§¯æ ¸ï¼Œå½¢çŠ¶ä¸º (kernel_size, kernel_size)
        stride: å·ç§¯æ­¥é•¿ï¼Œé»˜è®¤ä¸º3
        
    è¿”å›:
        å·ç§¯ç»“æœæ•°ç»„
    """
    # è·å–è¾“å…¥æ•°ç»„å’Œå·ç§¯æ ¸çš„å°ºå¯¸
    input_height, input_width = input_array.shape
    kernel_size = kernel.shape[0]
    
    # è®¡ç®—è¾“å‡ºæ•°ç»„çš„å°ºå¯¸
    output_height = (input_height - kernel_size) // stride + 1
    output_width = (input_width - kernel_size) // stride + 1
    
    # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„
    output = np.zeros((output_height, output_width))
    
    # æ‰§è¡Œå·ç§¯æ“ä½œ
    for i in range(output_height):
        for j in range(output_width):
            # è®¡ç®—å½“å‰çª—å£çš„ä½ç½®
            start_i = i * stride
            start_j = j * stride
            
            # æå–å½“å‰çª—å£
            window = input_array[start_i:start_i+kernel_size, start_j:start_j+kernel_size]
            
            # è®¡ç®—å·ç§¯å’Œ
            output[i, j] = np.sum(window * kernel)
    
    return output

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»º10x10çš„ç¤ºä¾‹è¾“å…¥æ•°ç»„
    input_array = np.ones((8, 8))
    # å³è¾“å…¥æ•°ç»„æ¯è¡Œåˆ—æ•°æ®ä¸‹æ ‡ä¸º 0-7 0-7
    
    # åˆ›å»º5x5çš„å·ç§¯æ ¸
    kernel = np.ones((5, 5))
    
    # æ‰§è¡Œå·ç§¯æ“ä½œï¼Œæ­¥å¹…ä¸º3
    # ç¬¬ä¸€æ¬¡å·ç§¯çš„åŒºåŸŸä¸º 0-4 0-4
    # ç¬¬äºŒæ¬¡å·ç§¯çš„åŒºåŸŸä¸º 3-7 3-7
    result = convolution_2d(input_array, kernel, stride=3)
    
    print("è¾“å…¥æ•°ç»„å½¢çŠ¶:", input_array.shape)
    print("å·ç§¯æ ¸å½¢çŠ¶:", kernel.shape)
    print("å·ç§¯ç»“æœå½¢çŠ¶:", result.shape)
    print("\nè¾“å…¥æ•°ç»„:")
    print(input_array)
    print("\nå·ç§¯æ ¸:")
    print(kernel)
    print("\nå·ç§¯ç»“æœ:")
    print(result)
'''
è¾“å…¥æ•°ç»„å½¢çŠ¶: (10, 10)
å·ç§¯æ ¸å½¢çŠ¶: (5, 5)
å·ç§¯ç»“æœå½¢çŠ¶: (2, 2)

è¾“å…¥æ•°ç»„:
[[1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]]

å·ç§¯æ ¸:
[[1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1.]]

å·ç§¯ç»“æœ:
[[25. 25.]
 [25. 25.]]
'''
```


### æ‰‹å†™æ•°å­—è¯†åˆ«


import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="torch" label="pytorch" default>
    è¿™æ˜¯pytorchå®ç°

```python showLineNumbers
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np

# åŠ è½½æ•°æ® & é¢„å¤„ç†
digits = load_digits()
X = digits.images.astype(np.float32) / 16.0  # å½’ä¸€åŒ–åˆ°0~1
y = digits.target
X = X[..., np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦ (n,8,8,1)
num_classes = 10

# åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶è°ƒæ•´ä¸ºNCHWæ ¼å¼
X_train = torch.tensor(X_train).permute(0, 3, 1, 2)  # NHWC -> NCHW
X_val = torch.tensor(X_val).permute(0, 3, 1, 2)
y_train = torch.tensor(y_train, dtype=torch.long)
y_val = torch.tensor(y_val, dtype=torch.long)


# å®šä¹‰æ¨¡å‹
class SimpleConvNet(nn.Module):
    def __init__(self):
        super(SimpleConvNet, self).__init__()
        self.conv = nn.Conv2d(1, 8, kernel_size=3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º8é€šé“
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2)  # 2x2æ± åŒ–
        self.fc = nn.Linear(4 * 4 * 8, num_classes)  # å…¨è¿æ¥å±‚

    def forward(self, x):
        x = self.conv(x)  # å·ç§¯å±‚
        x = self.relu(x)  # ReLUæ¿€æ´»
        x = self.pool(x)  # æœ€å¤§æ± åŒ–
        x = x.reshape(x.shape[0], -1)
        x = self.fc(x)  # å…¨è¿æ¥å±‚
        return x


# åˆ›å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = SimpleConvNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# è®­ç»ƒå‚æ•°
epochs = 10
batch_size = 64

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨è¿›è¡Œæ‰¹å¤„ç†
    indices = torch.randperm(len(X_train))

    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    total_loss = 0

    # å°æ‰¹é‡è®­ç»ƒ
    for i in range(0, len(X_train), batch_size):
        # è·å–æ‰¹æ¬¡æ•°æ®
        batch_indices = indices[i : i + batch_size]
        x_batch = X_train[batch_indices]
        y_batch = y_train[batch_indices]

        # å‰å‘ä¼ æ’­
        outputs = model(x_batch)
        loss = criterion(outputs, y_batch)
        total_loss += loss.item()

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()  # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°

    # éªŒè¯
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        outputs = model(X_val)
        _, predicted = torch.max(outputs, 1)  # è·å–æœ€å¤§å€¼æ‰€åœ¨ä½ç½®
        accuracy = (predicted == y_val).float().mean()

    print(
        f"Epoch {epoch+1}/{epochs}  loss={total_loss/len(indices)*batch_size:.4f}  val_acc={accuracy:.4f}"
    )
'''
Epoch 1/10  loss=2.2956  val_acc=0.4472
Epoch 2/10  loss=2.0604  val_acc=0.6167
Epoch 3/10  loss=1.5787  val_acc=0.7861
Epoch 4/10  loss=1.0126  val_acc=0.8000
Epoch 5/10  loss=0.6914  val_acc=0.7972
Epoch 6/10  loss=0.5458  val_acc=0.7917
Epoch 7/10  loss=0.4080  val_acc=0.8417
Epoch 8/10  loss=0.3853  val_acc=0.8778
Epoch 9/10  loss=0.3235  val_acc=0.9111
Epoch 10/10  loss=0.2700  val_acc=0.9250
'''
```
  </TabItem>
  <TabItem value="numpy" label="numpy">
    è¿™æ˜¯numpyå®ç°

```python showLineNumbers
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split


# å·ç§¯å±‚å‰å‘ä¼ æ’­
def conv2d_forward(x, w, b):
    # x: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N,H,W,C)
    # w: å·ç§¯æ ¸æƒé‡ï¼Œå½¢çŠ¶ä¸º(Kh,Kw,C,Cout)
    # b: åç½®é¡¹ï¼Œé•¿åº¦ä¸ºCout
    N, H, W, C = x.shape  # è·å–è¾“å…¥æ•°æ®çš„å½¢çŠ¶
    Kh, Kw, _, Cout = w.shape  # è·å–å·ç§¯æ ¸çš„å½¢çŠ¶
    padding = 1  # å›ºå®šä½¿ç”¨padding=1
    Ho = (H + 2 * padding - Kh) + 1  # è®¡ç®—è¾“å‡ºé«˜åº¦
    Wo = (W + 2 * padding - Kw) + 1  # è®¡ç®—è¾“å‡ºå®½åº¦
    # å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¡«å……
    x_pad = np.pad(
        x, [(0, 0), (padding, padding), (padding, padding), (0, 0)], "constant"
    )
    # åˆå§‹åŒ–è¾“å‡ºå¼ é‡
    y = np.zeros((N, Ho, Wo, Cout), dtype=x.dtype)

    # è®¡ç®—å·ç§¯
    for n in range(N):  # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(Ho):  # éå†è¾“å‡ºé«˜åº¦
            for j in range(Wo):  # éå†è¾“å‡ºå®½åº¦
                # è·å–å½“å‰ä½ç½®å¯¹åº”çš„è¾“å…¥æ•°æ®å—
                patch = x_pad[n, i : i + Kh, j : j + Kw, :]
                for cout in range(Cout):  # éå†æ¯ä¸ªè¾“å‡ºé€šé“
                    # è®¡ç®—å·ç§¯ç»“æœ
                    y[n, i, j, cout] = np.sum(patch * w[:, :, :, cout]) + b[cout]

    cache = (x, w, b, x_pad)  # ç¼“å­˜ç”¨äºåå‘ä¼ æ’­
    return y, cache


# å·ç§¯å±‚åå‘ä¼ æ’­
def conv2d_backward(dy, cache):
    # dy: è¾“å‡ºæ¢¯åº¦ï¼Œå½¢çŠ¶ä¸å·ç§¯å±‚è¾“å‡ºç›¸åŒ
    # cache: å‰å‘ä¼ æ’­ä¿å­˜çš„ç¼“å­˜æ•°æ®
    x, w, b, x_pad = cache  # è§£åŒ…ç¼“å­˜æ•°æ®
    N, H, W, C = x.shape  # è·å–è¾“å…¥æ•°æ®çš„å½¢çŠ¶
    Kh, Kw, _, Cout = w.shape  # è·å–å·ç§¯æ ¸çš„å½¢çŠ¶
    _, Ho, Wo, _ = dy.shape  # è·å–è¾“å‡ºæ¢¯åº¦çš„å½¢çŠ¶
    padding = 1  # å›ºå®šä½¿ç”¨padding=1

    # åˆå§‹åŒ–æ¢¯åº¦
    dx_pad = np.zeros_like(x_pad)  # å¡«å……åè¾“å…¥çš„æ¢¯åº¦
    dw = np.zeros_like(w)  # æƒé‡æ¢¯åº¦
    db = np.zeros_like(b)  # åç½®æ¢¯åº¦

    # è®¡ç®—æ¢¯åº¦
    for n in range(N):  # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(Ho):  # éå†è¾“å‡ºé«˜åº¦
            for j in range(Wo):  # éå†è¾“å‡ºå®½åº¦
                # è·å–å½“å‰ä½ç½®å¯¹åº”çš„è¾“å…¥æ•°æ®å—
                patch = x_pad[n, i : i + Kh, j : j + Kw, :]
                for cout in range(Cout):  # éå†æ¯ä¸ªè¾“å‡ºé€šé“
                    # ç´¯åŠ æƒé‡æ¢¯åº¦
                    dw[:, :, :, cout] += patch * dy[n, i, j, cout]
                    # ç´¯åŠ è¾“å…¥æ¢¯åº¦
                    dx_pad[n, i : i + Kh, j : j + Kw, :] += (
                        w[:, :, :, cout] * dy[n, i, j, cout]
                    )
                    # ç´¯åŠ åç½®æ¢¯åº¦
                    db[cout] += dy[n, i, j, cout]

    # å»é™¤å¡«å……ï¼Œå¾—åˆ°åŸå§‹è¾“å…¥æ¢¯åº¦
    dx = dx_pad[:, padding:-padding, padding:-padding, :]
    return dx, dw, db


# ReLUæ¿€æ´»å‡½æ•°å‰å‘ä¼ æ’­
def relu_forward(x):
    # x: è¾“å…¥æ•°æ®
    y = np.maximum(0, x)  # ReLUæ¿€æ´»å‡½æ•°ï¼šmax(0,x)
    return y, x  # è¿”å›è¾“å‡ºå’Œç¼“å­˜(è¾“å…¥x)


# ReLUæ¿€æ´»å‡½æ•°åå‘ä¼ æ’­
def relu_backward(dy, cache):
    # dy: è¾“å‡ºæ¢¯åº¦
    # cache: å‰å‘ä¼ æ’­ç¼“å­˜çš„è¾“å…¥x
    x = cache
    # ReLUæ¢¯åº¦ï¼šå½“x>0æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0
    return dy * (x > 0)


# æœ€å¤§æ± åŒ–å‰å‘ä¼ æ’­
def maxpool_forward(x):
    # x: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N,H,W,C)
    N, H, W, C = x.shape  # è·å–è¾“å…¥å½¢çŠ¶
    pool_size = (2, 2)  # å›ºå®šæ± åŒ–çª—å£å¤§å°ä¸º2x2
    ph, pw = pool_size
    # è®¡ç®—è¾“å‡ºå¤§å°
    Ho = (H - ph) // 2 + 1
    Wo = (W - pw) // 2 + 1

    # åˆå§‹åŒ–è¾“å‡ºå¼ é‡å’Œæ©ç 
    y = np.zeros((N, Ho, Wo, C), dtype=x.dtype)
    mask = {}  # è®°å½•æœ€å¤§å€¼ä½ç½®

    # è®¡ç®—æ± åŒ–
    for n in range(N):  # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(Ho):  # éå†è¾“å‡ºé«˜åº¦
            for j in range(Wo):  # éå†è¾“å‡ºå®½åº¦
                # è·å–å½“å‰æ± åŒ–çª—å£
                patch = x[n, i * 2 : i * 2 + ph, j * 2 : j * 2 + pw, :]
                # è®¡ç®—çª—å£å†…æœ€å¤§å€¼
                y[n, i, j, :] = patch.max(axis=(0, 1))

                # è®°å½•æ¯ä¸ªé€šé“çš„æœ€å¤§å€¼ä½ç½®
                for c in range(C):
                    idx = np.unravel_index(np.argmax(patch[:, :, c]), (ph, pw))
                    mask[(n, i, j, c)] = (i * 2 + idx[0], j * 2 + idx[1])

    return y, (x, mask)  # è¿”å›è¾“å‡ºå’Œç¼“å­˜


# æœ€å¤§æ± åŒ–åå‘ä¼ æ’­
def maxpool_backward(dy, cache):
    # dy: è¾“å‡ºæ¢¯åº¦
    # cache: å‰å‘ä¼ æ’­ä¿å­˜çš„ç¼“å­˜
    x, mask = cache  # è§£åŒ…ç¼“å­˜

    # åˆå§‹åŒ–è¾“å…¥æ¢¯åº¦
    dx = np.zeros_like(x)
    N, Ho, Wo, C = dy.shape  # è·å–è¾“å‡ºæ¢¯åº¦å½¢çŠ¶

    # è®¡ç®—æ¢¯åº¦ï¼šä»…åœ¨æœ€å¤§å€¼ä½ç½®ä¼ é€’æ¢¯åº¦
    for n in range(N):  # éå†æ¯ä¸ªæ ·æœ¬
        for i in range(Ho):  # éå†è¾“å‡ºé«˜åº¦
            for j in range(Wo):  # éå†è¾“å‡ºå®½åº¦
                for c in range(C):  # éå†æ¯ä¸ªé€šé“
                    # è·å–æœ€å¤§å€¼ä½ç½®å¹¶ä¼ é€’æ¢¯åº¦
                    xi, xj = mask[(n, i, j, c)]
                    dx[n, xi, xj, c] += dy[n, i, j, c]

    return dx


# å±•å¹³å±‚å‰å‘ä¼ æ’­
def flatten_forward(x):
    # x: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N,H,W,C)
    # å°†è¾“å…¥å±•å¹³ä¸º(N, H*W*C)çš„äºŒç»´å¼ é‡
    return x.reshape(x.shape[0], -1), x.shape


# å±•å¹³å±‚åå‘ä¼ æ’­
def flatten_backward(dy, cache):
    # dy: è¾“å‡ºæ¢¯åº¦ï¼Œå½¢çŠ¶ä¸º(N, H*W*C)
    # cache: åŸå§‹è¾“å…¥å½¢çŠ¶
    # å°†æ¢¯åº¦é‡å¡‘å›åŸå§‹è¾“å…¥å½¢çŠ¶
    return dy.reshape(cache)


# å…¨è¿æ¥å±‚å‰å‘ä¼ æ’­
def dense_forward(x, w, b):
    # x: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N, Din)
    # w: æƒé‡ï¼Œå½¢çŠ¶ä¸º(Din, Dout)
    # b: åç½®ï¼Œé•¿åº¦ä¸ºDout
    y = x.dot(w) + b  # çº¿æ€§å˜æ¢ï¼šy = xÂ·w + b
    return y, (x, w, b)  # è¿”å›è¾“å‡ºå’Œç¼“å­˜


# å…¨è¿æ¥å±‚åå‘ä¼ æ’­
def dense_backward(dy, cache):
    # dy: è¾“å‡ºæ¢¯åº¦ï¼Œå½¢çŠ¶ä¸º(N, Dout)
    # cache: å‰å‘ä¼ æ’­ç¼“å­˜
    x, w, b = cache  # è§£åŒ…ç¼“å­˜

    # è®¡ç®—å„ä¸ªå‚æ•°çš„æ¢¯åº¦
    dx = dy.dot(w.T)  # è¾“å…¥æ¢¯åº¦ï¼šdyÂ·w^T
    dw = x.T.dot(dy)  # æƒé‡æ¢¯åº¦ï¼šx^TÂ·dy
    db = dy.sum(axis=0)  # åç½®æ¢¯åº¦ï¼šæ¯ä¸ªæ‰¹æ¬¡æ¢¯åº¦çš„å’Œ

    return dx, dw, db


# Softmaxäº¤å‰ç†µæŸå¤±å‰å‘ä¼ æ’­
def softmax_crossentropy_forward(logits, labels):
    # logits: é¢„æµ‹å€¼ï¼Œå½¢çŠ¶ä¸º(N, ç±»åˆ«æ•°)
    # labels: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N, ç±»åˆ«æ•°)ï¼Œone-hotç¼–ç 

    # è®¡ç®—softmaxæ¦‚ç‡ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡º
    ex = np.exp(logits - logits.max(axis=1, keepdims=True))
    proba = ex / ex.sum(axis=1, keepdims=True)

    N = logits.shape[0]  # æ ·æœ¬æ•°é‡
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss = -np.sum(labels * np.log(proba + 1e-12)) / N

    return loss, (proba, labels, N)  # è¿”å›æŸå¤±å’Œç¼“å­˜


# Softmaxäº¤å‰ç†µæŸå¤±åå‘ä¼ æ’­
def softmax_crossentropy_backward(cache):
    # cache: å‰å‘ä¼ æ’­ç¼“å­˜
    proba, labels, N = cache  # è§£åŒ…ç¼“å­˜
    # è®¡ç®—æ¢¯åº¦ï¼š(softmaxæ¦‚ç‡ - çœŸå®æ ‡ç­¾) / æ ·æœ¬æ•°
    return (proba - labels) / N


# åŠ è½½æ•°æ®é›†
digits = load_digits()
X = digits.images.astype(np.float32) / 16.0  # å½’ä¸€åŒ–åˆ°0~1èŒƒå›´
y = digits.target
X = X[..., np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦ï¼Œå˜ä¸º(N,8,8,1)çš„å½¢çŠ¶
num_classes = 10

# å°†æ ‡ç­¾è½¬ä¸ºone-hotç¼–ç 
Y = np.eye(num_classes)[y]

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, stratify=y)
y_val_labels = Y_val.argmax(axis=1)  # éªŒè¯é›†æ ‡ç­¾(æ•°å­—å½¢å¼)

# åˆå§‹åŒ–ç½‘ç»œå‚æ•°(ä½¿ç”¨Heåˆå§‹åŒ–)
w1 = np.random.randn(3, 3, 1, 8).astype(np.float32) * np.sqrt(2 / 9)  # å·ç§¯å±‚æƒé‡
b1 = np.zeros(8, dtype=np.float32)  # å·ç§¯å±‚åç½®
w2 = np.random.randn(4 * 4 * 8, num_classes).astype(np.float32) * np.sqrt(
    2 / 128
)  # å…¨è¿æ¥å±‚æƒé‡
b2 = np.zeros(num_classes, dtype=np.float32)  # å…¨è¿æ¥å±‚åç½®

# è¶…å‚æ•°
epochs = 10  # è®­ç»ƒè½®æ•°
batch_size = 64  # æ‰¹æ¬¡å¤§å°
lr = 0.1  # å­¦ä¹ ç‡

# è®­ç»ƒè¿‡ç¨‹
num_train = X_train.shape[0]  # è®­ç»ƒæ ·æœ¬æ•°é‡
for ep in range(epochs):
    # æ‰“ä¹±è®­ç»ƒæ•°æ®
    perm = np.random.permutation(num_train)
    X_train = X_train[perm]
    Y_train = Y_train[perm]

    # å°æ‰¹é‡è®­ç»ƒ
    for i in range(0, num_train, batch_size):
        xb = X_train[i : i + batch_size]  # å½“å‰æ‰¹æ¬¡è¾“å…¥
        yb = Y_train[i : i + batch_size]  # å½“å‰æ‰¹æ¬¡æ ‡ç­¾

        # å‰å‘ä¼ æ’­
        out1, c1 = conv2d_forward(xb, w1, b1)  # å·ç§¯å±‚
        out1r, c1r = relu_forward(out1)  # ReLUæ¿€æ´»
        out2, c2 = maxpool_forward(out1r)  # æœ€å¤§æ± åŒ–
        flat, cf = flatten_forward(out2)  # å±•å¹³å±‚
        logits, c3 = dense_forward(flat, w2, b2)  # å…¨è¿æ¥å±‚
        loss, c4 = softmax_crossentropy_forward(logits, yb)  # æŸå¤±è®¡ç®—

        # åå‘ä¼ æ’­
        dlogits = softmax_crossentropy_backward(c4)  # æŸå¤±æ¢¯åº¦
        dflat, dw2, db2 = dense_backward(dlogits, c3)  # å…¨è¿æ¥å±‚æ¢¯åº¦
        dout2 = flatten_backward(dflat, cf)  # å±•å¹³å±‚æ¢¯åº¦
        dout1r = maxpool_backward(dout2, c2)  # æ± åŒ–å±‚æ¢¯åº¦
        dout1 = relu_backward(dout1r, c1r)  # ReLUæ¢¯åº¦
        _, dw1, db1 = conv2d_backward(dout1, c1)  # å·ç§¯å±‚æ¢¯åº¦

        # å‚æ•°æ›´æ–°(æ¢¯åº¦ä¸‹é™)
        w1 -= lr * dw1  # æ›´æ–°å·ç§¯å±‚æƒé‡
        b1 -= lr * db1  # æ›´æ–°å·ç§¯å±‚åç½®
        w2 -= lr * dw2  # æ›´æ–°å…¨è¿æ¥å±‚æƒé‡
        b2 -= lr * db2  # æ›´æ–°å…¨è¿æ¥å±‚åç½®

    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
    out1, _ = conv2d_forward(X_val, w1, b1)  # å·ç§¯å±‚å‰å‘ä¼ æ’­
    out1r, _ = relu_forward(out1)  # ReLUæ¿€æ´»
    out2, _ = maxpool_forward(out1r)  # æœ€å¤§æ± åŒ–
    flat, _ = flatten_forward(out2)  # å±•å¹³å±‚
    logits, _ = dense_forward(flat, w2, b2)  # å…¨è¿æ¥å±‚
    preds = np.argmax(logits, axis=1)  # é¢„æµ‹ç»“æœ
    acc = (preds == y_val_labels).mean()  # è®¡ç®—å‡†ç¡®ç‡
    print(f"è½®æ¬¡ {ep+1}/{epochs}  æŸå¤±={loss:.4f}  éªŒè¯å‡†ç¡®ç‡={acc:.4f}")
```
  </TabItem>

</Tabs>


## AlexNet

åœ¨2012å¹´ImageNetå›¾åƒè¯†åˆ«æŒ‘æˆ˜èµ›ä¸­å–å¾—äº†å·¨å¤§çªç ´ï¼Œæ˜¯æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçˆ†å‘çš„æ ‡å¿—ã€‚å®ƒæ¯”LeNet-5æ›´æ·±ã€æ›´å®½ï¼Œä½¿ç”¨äº†ReLUæ¿€æ´»å‡½æ•°ã€Dropoutç­‰æŠ€æœ¯ã€‚

è¯æ˜äº†GPUå¯ä»¥å¤§å¹…æå‡ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦,è¯æ˜äº†æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚

:::info
ã€ŠImageNet Classification with Deep Convolutional Neural Networksã€‹æˆªæ­¢2025å¹´ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬3ã€‚
:::

### ReLUæ¿€æ´»å‡½æ•°

### Dropout

## ResNet

ResNetå¼•å…¥äº†â€œæ®‹å·®è¿æ¥â€ï¼ˆResidual Connectionï¼‰ï¼Œè§£å†³äº†æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ¢¯åº¦æ¶ˆå¤±å’Œç½‘ç»œé€€åŒ–çš„é—®é¢˜ï¼Œä½¿å¾—å¯ä»¥è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œï¼ˆä¾‹å¦‚152å±‚ï¼‰ã€‚

æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸæœ€é‡è¦çš„çªç ´ä¹‹ä¸€ï¼Œä½¿å¾—è®­ç»ƒè¶…æ·±ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œæå¤§åœ°æå‡äº†å›¾åƒè¯†åˆ«çš„å‡†ç¡®ç‡ã€‚

:::info
ã€ŠDeep Residual Learning for Image Recognitionã€‹æˆªæ­¢2025å¹´ï¼Œæœºå™¨è§†è§‰æ ‡ç­¾ä¸‹ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬1ã€‚
:::

### æ®‹å·®è¿æ¥

## Adam

æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”è‡ªé€‚åº”çš„éšæœºä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡ç»“åˆä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰çš„ä¼°è®¡ï¼Œä¸ºæ¯ä¸ªæ¨¡å‹å‚æ•°ç‹¬ç«‹è°ƒæ•´å­¦ä¹ ç‡ã€‚

å¤§å¹…ç®€åŒ–äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå‡å°‘äº†æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡çš„éœ€è¦ï¼Œå¹¶ä¿è¯äº†åœ¨ç¨€ç–æ¢¯åº¦ï¼ˆå°¤å…¶æ˜¯åœ¨ NLP ä¸­ï¼‰ä¸‹çš„ç¨³å®šæ”¶æ•›ã€‚

æˆä¸ºæ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œ Transformer æ¶æ„çš„é»˜è®¤ä¼˜åŒ–å™¨ä¹‹ä¸€ã€‚

:::info
ã€ŠAdam: A Method for Stochastic Optimizationã€‹æˆªæ­¢2025å¹´ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬6ã€‚
:::

## Batch Normalizations

æå‡ºäº†ä¸€ç§è§„èŒƒåŒ–ç½‘ç»œå±‚è¾“å…¥çš„æ–¹æ³•ï¼Œè§£å†³äº†è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶â€œå†…éƒ¨åå˜é‡åç§»â€ï¼ˆInternal Covariate Shiftï¼‰çš„é—®é¢˜ï¼Œå³ä¸­é—´å±‚è¾“å…¥çš„åˆ†å¸ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–çš„ç°è±¡ã€‚

ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡å’Œæ›´æ·±çš„ï¼ˆæ›´å¤æ‚çš„ï¼‰ç½‘ç»œæ¶æ„è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶æå¤§åœ°åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚

æœ‰æ•ˆå……å½“æ­£åˆ™åŒ–å™¨ï¼Œå‡å°‘äº†å¯¹ Dropout ç­‰å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯çš„ä¾èµ–ã€‚

:::info
ã€ŠBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftã€‹æˆªæ­¢2025å¹´ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬8ã€‚
:::

## LSTM

:::info
LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€é‡è¦çš„æ¶æ„ä¹‹ä¸€ã€‚1997 å¹´ç”± Hochreiter å’Œ Schmidhuber æå‡ºï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£å†³ä¼ ç»Ÿ RNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œèƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚

**LSTM çš„æ ¸å¿ƒåˆ›æ–°ï¼š**
- **é—¨æ§æœºåˆ¶**ï¼šé€šè¿‡é—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨æ§åˆ¶ä¿¡æ¯æµåŠ¨
- **ç»†èƒçŠ¶æ€**ï¼šä¿¡æ¯é«˜é€Ÿå…¬è·¯ï¼Œè®©æ¢¯åº¦èƒ½å¤Ÿé•¿è·ç¦»ä¼ æ’­
- **é•¿æœŸè®°å¿†**ï¼šæœ‰æ•ˆæ•è·åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»

ã€ŠLong Short-Term Memoryã€‹è®ºæ–‡æˆªæ­¢ 2025 å¹´ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬ 5ï¼Œæ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸæœ€å…·å½±å“åŠ›çš„è®ºæ–‡ä¹‹ä¸€ã€‚

[åŸå§‹è®ºæ–‡ï¼šLong Short-Term Memory (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)
:::

## Vision Transformer

åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘éƒ½æ˜¯åºåˆ—ã€‚

`Vision Transformer`ï¼ˆViTï¼‰é¦–æ¬¡æˆåŠŸå°† Transformer æ¶æ„ï¼ˆåŸç”¨äº NLPï¼‰åº”ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ŒæŒ‘æˆ˜äº† CNN åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸»å¯¼åœ°ä½ã€‚

`ViT`çš„æˆåŠŸæ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸæ–¹æ³•è®ºçš„ä¸€æ¬¡é‡å¤§è½¬å˜ï¼Œæ ‡å¿—ç€â€œå¤§ä¸€ç»Ÿâ€æ¶æ„ï¼ˆå³ `Transformer`ï¼‰å¼€å§‹ç»Ÿæ²» NLP å’Œ CV ä¸¤ä¸ªé¢†åŸŸã€‚

:::info
ã€ŠAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleã€‹åœ¨ 2020 å¹´åŠä¹‹åå‘è¡¨çš„è®ºæ–‡ä¸­ï¼Œè°·æ­Œå­¦æœ¯æ€»å¼•ç”¨æ¬¡æ•°æ’åç¬¬1ã€‚
:::

