---
sidebar_position: 3
title: 线性回归
---

## 线性回归

线性回归是利用连续性变量来估计实际数值（例如房价，呼叫次数和总销售额等）。

我们通过线性回归算法找出自变量和因变量间的最佳线性关系，图形上可以确定一条最佳直线。

这条最佳直线就是回归线。这个回归关系可以用$Y=aX+b$表示。

多个数据可以用$Y= β0X0 + β1X1 + β2X2+…… βnXn+ ε $表示。

## 评估数据的离散程度

平均值：数据相加除以数据个数

平均差：数据与平均值的差的绝对值相加除以数据个数

均方误差：数据与平均值的差的平方相加除以数据个数

| 数据1 | 数据2 | 平均值 | 平均差 | 均方误差 |
|--------|--------|--------|--------|----------|
| 0      | 0      | 0      | 0      | 0        |
| -4     | 4      | 0      | 4      | 16       |
| 7      | 1      | 4      | 4      | 25       |

我们预期中，理想效果应该是 0、0 好于 -4、4 好于 7、1。只有均方误差正确的反应了这一点。

通过误差的大小，我们可以慢慢修正我们的参数让线性拟合更好，导数可以反应数据变化的趋势，所以我们可以求导来修改参数。

## 求导

这个过程也叫：求梯度、反向传播（狭义）

误差我们用的是均方误差：求了每个数据与平均值的差的平方，再加和，再求平均

每个数据的误差我们一般叫**损失**，写作$loss$，他的值等于数据集中的目标值，减去我们线性公式算出来的预测值。

即：$loss = (y - (wx + b))^2$ 

这个表达式中，loss是算出来的，y是目标值，源自数据集，x是特征值，源自数据集。

我们调整w和b的值就可以间接控制loss的值变大或变小（这里主要是期望变小）。

我想既更新w，也更新b。

所以分2次求导，第一次求：$\frac{\partial loss}{\partial w}$ 

第二次求： $\frac{\partial loss}{\partial b}$。


```python showLineNumbers
import numpy as np
from matplotlib import pyplot as plt


class Line:
    def __init__(self, data):
        self.w = 1
        self.b = 0
        self.learning_rate = 0.01
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []

    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]

    def predict(self, x):
        return self.w * x + self.b
    
    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.predict(x)
                # Calculate gradients
                gradient_w = -2 * x * (y - y_pred)
                gradient_b = -2 * (y - y_pred)
                # Update weights
                self.w -= self.learning_rate * gradient_w
                self.b -= self.learning_rate * gradient_b
                # Calculate loss
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()

    def plot(self):
        plt.ion()  # Enable interactive mode
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.predict(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # Input data
    data = [(1, 1), (1.8, 2), (2.5, 3), (4.2, 4), (5, 5), (6, 6), (7, 7)]
    s = Line(data)
    s.get_data(data)
    s.train(100)
```

## 使用sklearn模块完成

```python showLineNumbers
import numpy as np
from sklearn.linear_model import LinearRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([2, 3, 4, 4, 6])  # 因变量
# 创建线性回归模型
model = LinearRegression()

# 拟合模型
model.fit(X, y)

# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)


# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
predicted_value = model.predict(new_data_point)
print("预测值:", predicted_value)

```

### 简单示例

```python showLineNumbers
from sklearn import datasets
from sklearn.linear_model import LinearRegression

# .fetch_california_housing() 加载加利福尼亚州住房数据集
loaded_data = datasets.fetch_california_housing()
# .data 数据集中的特征数据
data_X = loaded_data.data
# .target 数据集中的标签数据
data_y = loaded_data.target
# 创建线性回归模型
model = LinearRegression()
# 拟合模型
# .fit() 方法接受两个参数：特征数据和标签数据
model.fit(data_X, data_y)

# 预测前四所房屋价格
# .predict() 方法接受一个参数：特征数据
print(model.predict(data_X[:4, :]))
# 真实价格
print(data_y[:4])
```

### 效果评估

```python showLineNumbers
print(model.get_params())# 获取模型参数
# //{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}
print(model.score(data_X, data_y))
# // 0.606232685199805
# 这意味着数据集中因变量的 60% 的变异性已得到考虑，而其余 40% 的变异性仍未得到解释。
```

### 查看模型属性

```python showLineNumbers
# 打印回归系数和截距
print("回归系数 (斜率):", model.coef_)
print("截距:", model.intercept_)
```

## 损失函数拓展

| 损失函数名称         | 适用场景         | 数学形式                                                              | 特点                                                         |
|----------------------|------------------|-----------------------------------------------------------------------|--------------------------------------------------------------|
| 交叉熵损失（Cross Entropy） | 多分类，常配合Softmax | $L = -\sum_{i} y_i \log(\hat{y}_i)$                                     | 最常用，适合 one-hot 标签，梯度清晰，收敛快                   |
| MSE（均方误差）       | 少见于分类任务    | $L = \frac{1}{n} \sum_{i} (y_i - \hat{y}_i)^2$                          | 简单直观，但不适合分类，会导致梯度问题                       |
| KL散度（KL Divergence） | 预测分布 vs 真分布 | $L = \sum_i y_i \log\left(\frac{y_i}{\hat{y}_i}\right)$                | 衡量分布差异，常用于模型输出概率分布                        |
| Focal Loss           | 类别不均衡时      | $L = -\alpha(1 - \hat{y}_t)^\gamma \log(\hat{y}_t)$                     | 抑制易分类样本的损失，对困难样本关注更多                    |
| Hinge Loss（合页损失）| SVM等场景         | $L = \sum_i \max(0, 1 - y_i \cdot \hat{y}_i)$                          | 主要用于“硬间隔”分类问题，不常用于深度学习中的多分类任务 |

