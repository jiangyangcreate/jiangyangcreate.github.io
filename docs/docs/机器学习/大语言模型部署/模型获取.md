---
sidebar_position: 1
title: ğŸš§æ¨¡å‹è·å–
---

## å¼€æºç¤¾åŒº

å¤§æ¨¡å‹ç¤¾åŒºæ˜¯æŒ‡å›´ç»•å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ GPT ç³»åˆ—ã€BERTã€T5 ç­‰ï¼‰æ„å»ºçš„å¼€æ”¾åä½œå¹³å°å’Œç”Ÿæ€ç³»ç»Ÿã€‚è¿™äº›ç¤¾åŒºç”±ç ”ç©¶äººå‘˜ã€å¼€å‘è€…ã€æ•°æ®ç§‘å­¦å®¶ã€å·¥ç¨‹å¸ˆåŠçˆ±å¥½è€…ç»„æˆï¼Œä»–ä»¬å…±åŒè‡´åŠ›äºå¤§æ¨¡å‹çš„ç ”ç©¶ã€å¼€å‘ã€ä¼˜åŒ–å’Œåº”ç”¨ã€‚

ç°åœ¨æ¨¡å‹éå¸¸å¤šï¼Œå„æœ‰åƒç§‹ï¼Œä¸”æ›´æ–°è¿­ä»£éå¸¸å¿«ã€‚ä¸‹é¢çš„è¡¨æ ¼åˆ—å‡ºäº†éƒ¨åˆ†å…¬å¸åŠå…¶zä¸»è¦å¤§æ¨¡å‹ä»£å·ï¼š

| **å…¬å¸åç§°**                  | **å¤§æ¨¡å‹ä»£å·**      |
| ----------------------------- | ------------------- |
| **OpenAI**                    | GPT                 |
| **Meta**                      | Llama               |
| **Anthropic(å‰ OpenAI æˆå‘˜)** | Claude              |
| **X**                         | Grok                |
| **è°·æ­Œ**                      | Gemini              |
| **å¾®è½¯**                      | Phi                 |
| **ç™¾åº¦**                      | æ–‡å¿ƒå¤§æ¨¡å‹ (Ernie)  |
| **é˜¿é‡Œå·´å·´**                  | é€šä¹‰åƒé—® (Qwen), M6 |
| **è…¾è®¯**                      | æ··å…ƒ (Hunyuan)      |
| **å­—èŠ‚è·³åŠ¨**                  | è±†åŒ…                |
| **åä¸º**                      | ç›˜å¤å¤§æ¨¡å‹ (Pangu)  |

ç¤¾åŒºå…·æœ‰æ˜æ˜¾çš„é©¬å¤ªæ•ˆåº”ï¼Œå³å¤´éƒ¨æ•ˆåº”æ˜æ˜¾ï¼Œå¤´éƒ¨æ¨¡å‹æ‹¥æœ‰æœ€å¤šçš„èµ„æºï¼Œæœ€æ–°çš„æŠ€æœ¯ï¼Œæœ€å¤šçš„ç”¨æˆ·ã€‚è¿™é‡Œåˆ—ä¸¾ä¸¤ä¸ªåœ¨å›½å†…å¤–æœ‰ä¸€å®šå½±å“åŠ›çš„ç¤¾åŒºã€‚

### Hugging Face

ç¤¾åŒºåœ°å€ï¼š[https://huggingface.co/](https://huggingface.co/)

ä»¥ Qwen æ¨¡å‹ä¸ºä¾‹ï¼Œä¸‹é¢å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Hugging Face çš„ transformers åº“è¿›è¡Œæ¨ç†ã€‚å…¶ä¸­`model_name`ä¸ºæ¨¡å‹åœ°å€

```python showLineNumbers
from transformers import AutoModelForCausalLM, AutoTokenizer

model_size = "3B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("è¾“å…¥ä½ çš„é—®é¢˜: ")
    if prompt == "é€€å‡º":
        break

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œç”±é˜¿é‡Œå·´å·´äº‘åˆ›å»ºã€‚ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä½ æ€»æ˜¯ä»¥ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### é­”æ­ç¤¾åŒºï¼ˆé˜¿é‡Œè¾¾æ‘©é™¢ï¼‰

ç¤¾åŒºåœ°å€ï¼š[https://www.modelscope.cn/](https://www.modelscope.cn/)

é™¤äº† Hugging Face çš„ transformers åº“ï¼Œé­”æ­ç¤¾åŒºè¿˜æä¾›äº† modelscope åº“ï¼ŒåŸºäºä¸­å›½ç½‘ç»œç¯å¢ƒï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¨ç†ã€‚ä»£ç åŸºæœ¬ä¸ Hugging Face ä¸€è‡´ã€‚

```python showLineNumbers
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_size = "0.5B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("è¾“å…¥ä½ çš„é—®é¢˜: ")
    if prompt == "é€€å‡º":
        break

    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œç”±é˜¿é‡Œå·´å·´äº‘åˆ›å»ºã€‚ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä½ æ€»æ˜¯ä»¥ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

## å•†ç”¨æ¥å£

æ¥å£å¤§åŒå°å¼‚ï¼Œè¿™é‡Œåˆ—ä¸¾ä¸€ä¸ªå›½å†…çš„æ¥å£ä¸ä¸€ä¸ªå›½å¤–çš„æ¥å£ç”¨ä½œç¤ºä¾‹ã€‚

### OpenAI

åœ°å€ï¼š[https://openai.com/](https://openai.com/)

### ç™¾åº¦

åœ°å€ï¼š[https://cloud.baidu.com/](https://cloud.baidu.com/)
