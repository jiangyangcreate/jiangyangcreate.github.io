---
sidebar_position: 1
title: 模型社区与部署
---

## 开源社区

大模型社区是指围绕大型深度学习模型构建的开放协作平台和生态系统，除了模型还提供：数据集、教程、体验等功能。这些社区由研究人员、开发者、数据科学家、工程师及爱好者组成，他们共同致力于大模型的研究、开发、优化和应用。

下面的表格列出了部分公司及其主要大模型代号，排名不分先后：

| **公司名称**  |   **大模型代号**   |
| :-----------: | :----------------: |
| **Anthropic** |       Claude       |
| **阿里巴巴**  |  通义千问 (Qwen)   |
|  **OpenAI**   |        GPT         |
|   **谷歌**    |       Gemini       |
| **深度求索**  |      DeepSeek      |
| **月之暗面**  |        KIMI        |
|    **xAI**    |        Grok        |
|   **智谱**    |   智谱清言 (GLM)   |
|   **微软**    |        Phi         |
|   **Meta**    |       Llama        |
| **字节跳动**  |    豆包（coze）    |
|   **百度**    | 文心大模型 (Ernie) |
|   **腾讯**    |   混元 (Hunyuan)   |
|   **华为**    | 盘古大模型 (Pangu) |

社区具有明显的马太效应，即头部效应明显，头部模型拥有最多的资源，最新的技术，最多的用户。这里列举两个在国内外有一定影响力的社区。

### Hugging Face

社区地址：[https://huggingface.co/](https://huggingface.co/)

以 Qwen 模型为例，下面展示如何使用 Hugging Face 的 transformers 库进行推理。其中`model_name`为模型地址

```python showLineNumbers
from transformers import AutoModelForCausalLM, AutoTokenizer

model_size = "3B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### 魔搭社区（阿里达摩院）

社区地址：[https://www.modelscope.cn/](https://www.modelscope.cn/)

除了 Hugging Face 的 transformers 库，魔搭社区还提供了 modelscope 库，基于中国网络环境，可以方便地进行推理。代码除了开头的导包部分，剩下的与 Hugging Face 一致。

```python showLineNumbers
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_size = "0.5B"  # 3B 7B 14B 32B
model_name = f"Qwen/Qwen2.5-{model_size}-Instruct"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

while True:
    prompt = input("输入你的问题: ")
    if prompt == "退出":
        break

    messages = [
        {
            "role": "system",
            "content": "你是一个AI助手，由阿里巴巴云创建。你是一个乐于助人的助手。你总是以中文回答问题。",
        },
        {"role": "user", "content": prompt},
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_input = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(**model_input, max_new_tokens=512)
    generated_ids = [output[len(input_ids):] for input_ids, output in zip(model_input.input_ids, generated_ids)]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
```

### 模型常见后缀

大部分模型基本有代号、模型类型和参数大小三部分组成，例如：`Qwen2.5-Coder-3B`，其中`Qwen2.5`为代号，`Coder`为模型类型，`3B`为参数大小。

#### 模型类型

- `Instruct`：指令优化  
- `Fine-tuned`：领域微调  
- `Base`：基础模型  
- `Chat`：对话优化  
- `RAG`：检索增强生成  
- `Sparse`：稀疏模型  
- `Dense`：密集模型  
- `Multimodal`：多模态输入  
- `Enhanced` / `Extended`：功能改进或扩展  
- `Lite` / `Compact`：轻量化模型  
- `Instruction-following (IF)`：指令执行优化  
- `Code` / `Coder`：代码优化  
- `Knowledge`：知识问答优化  
- `Custom` / `Domain-Specific`：定制领域模型  
- `Aligned`：人类反馈对齐（通过使用人类反馈调整和优化后的模型）

#### 模型微调后缀

- `GGUF`：高效二进制通用量化格式
- `AWQ`：微调后量化：适应不同场景
- `GPTQ`：分组量化：精度换速度

#### 量化精度

- INT8 (8-bit Integer)：INT8通过将模型权重量化为8位整数，显著减少存储和计算需求，适用于大部分推理任务，但会带来一定的精度损失。
- INT4 (4-bit Integer)：INT4进一步减少存储和计算需求，能提供更快的推理速度和更低的内存占用，但精度损失更大，且支持较为有限。
- FP16 (16-bit Floating Point)：FP16通过使用16位浮点数减少存储需求，精度损失较小，但计算开销高于INT8和INT4，适合精度要求较高的任务。

## 模型部署

社区上默认的部署方式往往是用来测试，生产环境下我们往往需要：并发高、延迟低、占用小。同时兼顾不同的底层硬件。

解决方案对应Ollama、VLLM。

| 维度                 | Ollama                                     | VLLM                                           |
| -------------------- | ------------------------------------------ | ---------------------------------------------- |
| **核心功能**         | 模型管理和推理框架，支持快速加载、切换模型 | 高性能推理引擎，优化 Transformer 模型推理      |
| **主要特点**         | 易用的命令行工具，支持多个预训练模型       | 动态批次合并（dynamic batching），高吞吐量推理 |
| **性能**             | 适中，重点在于易用性                       | 高，专为分布式和高吞吐量推理优化               |
| **支持硬件**         | CPU/GPU                                    | GPU                                            |
| **异步支持**         | 支持多任务异步加载模型                     | 原生支持                                       |
| **扩展性**           | 易于集成到 Python 应用中，支持 REST API    | 面向大规模分布式推理设计，扩展性强             |
| **生态和社区**       | 新兴工具，生态尚在发展                     | 成长中，已有一定的研究和工业实践支持           |
| **学习曲线**         | 低，简单易用，适合快速上手                 | 中，适合熟悉分布式推理的用户                   |
| **部署难度**         | 低，支持简单命令行部署                     | 中，需要配置分布式推理环境                     |
| **模型兼容性**       | 支持多个预训练模型，如 GPT 类              | 支持多种 Transformer 模型，如 GPT、BERT        |
| **内存优化**         | 支持基础内存管理                           | 通过动态批次和显存优化提高吞吐量               |
| **文档和支持**       | 提供官方文档，支持 CLI 和简单的 REST 接口  | 提供详细文档，面向研究和工业实践               |
| **适用场景**         | 小型项目、模型快速切换、开发测试           | 高性能推理、分布式推理、服务大规模用户         |
| **对 Python 的支持** | 支持，通过 CLI 或 REST API 与 Python 集成  | 强支持，直接集成到 Python 项目                 |
| **成熟度**           | 新兴工具，功能逐步完善，适合单人AI问答     | 工业级项目，专注高性能推理                     |


### ollma

github地址：https://github.com/ollama/ollama

官网：https://ollama.com/

常见问题：https://github.com/ollama/ollama/blob/main/docs/faq.md

#### ollama部署huggingface模型

```bash showLineNumbers
ollama run hf.co/{username}/{reponame}:latest
```

示例1:运行最新的模型
```bash showLineNumbers
ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
```

示例2:运行特定的量化模型
```bash showLineNumbers
ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:Q8_0
```

### vllm

github地址：https://github.com/vllm-project/vllm

官网：https://vllm.ai/

#### vllm部署huggingface模型

```bash showLineNumbers
vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123
```

#### vllm部署本地模型

示例1：调用8卡推理
```bash showLineNumbers
vllm serve /home/ly/qwen2.5/Qwen2.5-32B-Instruct/ --tensor-parallel-size 8 --dtype auto --api-key 123 --gpu-memory-utilization 0.95 --max-model-len 27768  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2.5-32B-Instruct --kv-cache-dtype fp8_e5m2
```
示例2：指定某块GPU运行模型
```bash showLineNumbers
CUDA_VISIBLE_DEVICES=2 vllm serve /home/ly/qwen2.5/Qwen2-VL-7B-Instruct --dtype auto --tensor-parallel-size 1 auto --api-key 123 --gpu-memory-utilization 0.5 --max-model-len 5108  --enable-auto-tool-choice --tool-call-parser hermes --served-model-name Qwen2-VL-7B-Instruct --port 1236
```

Vllm不支持启动一个服务就可以随机切换其他模型（ollama支持）。

你通常需要为每一个模型单独运行一次vllm命令，并且每个模型都要提供不同的端口，比如他默认的是8000端口，而我上一个命令使用的是1236端口

