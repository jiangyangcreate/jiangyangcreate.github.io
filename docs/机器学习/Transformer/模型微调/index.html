<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-docs docs-version-current docs-doc-page docs-doc-id-机器学习/Transformer/模型微调" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">模型微调 | jiangmiemie</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jiangmiemie.com/pages/case/jiangmiemie.webp"><meta data-rh="true" name="twitter:image" content="https://jiangmiemie.com/pages/case/jiangmiemie.webp"><meta data-rh="true" property="og:url" content="https://jiangmiemie.com/docs/机器学习/Transformer/模型微调/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="docusaurus,blog, python, 开源"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-docs-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-docs-current"><meta data-rh="true" property="og:title" content="模型微调 | jiangmiemie"><meta data-rh="true" name="description" content="模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。"><meta data-rh="true" property="og:description" content="模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。"><link data-rh="true" rel="icon" href="/favicon.ico"><link data-rh="true" rel="canonical" href="https://jiangmiemie.com/docs/机器学习/Transformer/模型微调/"><link data-rh="true" rel="alternate" href="https://jiangmiemie.com/docs/机器学习/Transformer/模型微调/" hreflang="en"><link data-rh="true" rel="alternate" href="https://jiangmiemie.com/docs/机器学习/Transformer/模型微调/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://B8DUWB4CMX-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="jiangmiemie" href="/opensearch.xml">
<link rel="icon" href="/img/logo-512.svg">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37, 194, 160)">


<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="jiangmiemie RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="jiangmiemie Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="jiangmiemie JSON Feed">


<link rel="stylesheet" href="/katex/katex.min.css"><link rel="stylesheet" href="/assets/css/styles.11098515.css">
<script src="/assets/js/runtime~main.5a6c1700.js" defer="defer"></script>
<script src="/assets/js/main.ac43c03f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/">开发</a><a class="navbar__item navbar__link" href="/read/">书架</a><a class="navbar__item navbar__link" href="/blog/archive/">博文</a><a class="navbar__item navbar__link" href="/case/">个案</a><a class="navbar__item navbar__link" href="/gallery/">相簿</a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/">编程外的基础</a><button aria-label="Expand sidebar category &#x27;编程外的基础&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/选择编程语言/">选择编程语言</a><button aria-label="Expand sidebar category &#x27;选择编程语言&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/机器学习/">机器学习</a><button aria-label="Collapse sidebar category &#x27;机器学习&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/分类算法/">分类算法</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/聚类算法/">聚类算法</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/降维算法/">降维算法</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/回归算法/">回归算法</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/决策树/">决策树</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/经典神经网络/">经典神经网络</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/机器学习/计算机视觉/">计算机视觉</a><button aria-label="Expand sidebar category &#x27;计算机视觉&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/机器学习/自然语言处理/">自然语言处理</a><button aria-label="Expand sidebar category &#x27;自然语言处理&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/docs/机器学习/Transformer/">Transformer</a><button aria-label="Collapse sidebar category &#x27;Transformer&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/Transformer/Encoder-Decoder结构/">Encoder-Decoder结构</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/Transformer/注意力机制/">注意力机制</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/Transformer/Transformer结构/">Transformer结构</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/Transformer/手写多模态大模型/">手写多模态大模型</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/机器学习/Transformer/分布式训练/">分布式训练</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/机器学习/Transformer/模型微调/">模型微调</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/大模型的应用/">大模型的应用</a><button aria-label="Expand sidebar category &#x27;大模型的应用&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/后端通识/">后端通识</a><button aria-label="Expand sidebar category &#x27;后端通识&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/闲来无事/">闲来无事</a><button aria-label="Expand sidebar category &#x27;闲来无事&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/机器学习/"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/机器学习/Transformer/"><span itemprop="name">Transformer</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">模型微调</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>模型微调</h1></header><p>模型微调（Fine-Tuning） 是指在一个预训练的基础模型上，使用特定领域或特定任务的数据进行进一步训练，以使模型能够在特定任务上表现得更好。例如对计算机科学的名词翻译进行微调，可以提高翻译的准确性。</p>
<p>LLaMA-Factory 是基于 LLaMA 的模型微调框架，支持 LoRA 微调、冻结层微调、全量微调。既可以通过 WebUI 微调，也可以通过命令行微调。</p>
<p>项目地址：<a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener noreferrer">https://github.com/hiyouga/LLaMA-Factory</a></p>
<p>安装</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">cd LLaMA-Factory</span><br></span><span class="token-line" style="color:#000000"><span class="token plain">pip install --no-deps -e .</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>启动 WebUI</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token plain">llamafactory-cli webui</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>在操作界面我们会看到许多可选项，下面是一些常用选项的说明。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="微调">微调<a href="#微调" class="hash-link" aria-label="Direct link to 微调" title="Direct link to 微调">​</a></h2>
<p>微调有三种方法：LoRA 微调、冻结层微调、全量微调。</p>
<p>以微调一个 <strong>14B 参数</strong>的大型语言模型为例，以下是对三种常见微调方法的硬件需求和大致时间的估算：</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-微调">LoRA 微调<a href="#lora-微调" class="hash-link" aria-label="Direct link to LoRA 微调" title="Direct link to LoRA 微调">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：~24GB 显存以上即可（如 NVIDIA RTX 3090 或 A100）。LoRA 主要通过冻结大部分模型参数，仅在少量层中插入低秩适配模块，因此显存需求较低。</li>
<li><strong>GPU 数量</strong>：单卡或 2 卡即可应付中等规模的微调任务。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>（如数十万条数据，100k steps 以内）：几个小时到 1 天。</li>
<li><strong>大规模数据</strong>（如百万条数据，300k steps）：2-3 天。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>高效，显存需求低，适合个人开发者或中小型实验室。</li>
<li>微调后的模型参数（LoRA 插件）仅几百 MB，方便存储和共享。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="冻结层微调freeze">冻结层微调（Freeze）<a href="#冻结层微调freeze" class="hash-link" aria-label="Direct link to 冻结层微调（Freeze）" title="Direct link to 冻结层微调（Freeze）">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：需要 48GB 以上显存（如 A6000 或 80GB A100）。冻结大部分模型参数，微调后几层或部分特定模块。</li>
<li><strong>GPU 数量</strong>：1-4 张高性能 GPU。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>：1-2 天。</li>
<li><strong>大规模数据</strong>：3-5 天。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>显存需求较低，性能适中。</li>
<li>微调时间比全量微调短，但仍需较多资源。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="全量微调full-fine-tuning">全量微调（Full Fine-tuning）<a href="#全量微调full-fine-tuning" class="hash-link" aria-label="Direct link to 全量微调（Full Fine-tuning）" title="Direct link to 全量微调（Full Fine-tuning）">​</a></h3>
<p><strong>硬件需求</strong>：</p>
<ul>
<li><strong>显存</strong>：需 80GB 显存以上的高性能 GPU（如 NVIDIA A100 或 H100）。14B 参数模型通常需要 4 卡或更多 GPU 的分布式训练。</li>
<li><strong>GPU 数量</strong>：4-8 张 A100（或等效）GPU。对于超大模型，可能需要更多。</li>
</ul>
<p><strong>时间估算</strong>：</p>
<ul>
<li><strong>小规模数据</strong>：2-3 天。</li>
<li><strong>大规模数据</strong>：5 天到 1 周甚至更长。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>微调灵活性最高，可以针对特定任务完全优化模型。</li>
<li>模型质量可能略高于 LoRA 和冻结层微调。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>显存需求高，成本昂贵。</li>
<li>微调后模型体积巨大，通常需要数百 GB 存储。</li>
</ul>
<p>有些框架可以以时间换空间，例如使用梯度检查点：在显存不足时保存中间计算结果，降低瞬时显存需求。或者以精度换空间，例如使用混合精度训练：可以显著减少显存需求和训练时间。有兴趣可以自行搜索。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="量化">量化<a href="#量化" class="hash-link" aria-label="Direct link to 量化" title="Direct link to 量化">​</a></h2>
<p> 随着语言模型规模的不断增大，其训练的难度和成本已成为共识。 而随着用户数量的增加，模型推理的成本也在不断攀升，甚至可能成为限制模型部署的首要因素。 因此，我们需要对模型进行压缩以加速推理过程，而模型量化是其中一种有效的方法。</p>
<p>大语言模型的参数通常以高精度浮点数存储，这导致模型推理需要大量计算资源。 量化技术通过将高精度数据类型存储的参数转换为低精度数据类型存储， 可以在不改变模型参数量和架构的前提下加速推理过程。这种方法使得模型的部署更加经济高效，也更具可行性。</p>
<p>浮点数一般由 3 部分组成：符号位、指数位和尾数位。指数位越大，可表示的数字范围越大。尾数位越大、数字的精度越高。</p>
<p>量化可以根据何时量化分为：后训练量化和训练感知量化，也可以根据量化参数的确定方式分为：静态量化和动态量化。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qat-感知量化训练">QAT: 感知量化训练<a href="#qat-感知量化训练" class="hash-link" aria-label="Direct link to QAT: 感知量化训练" title="Direct link to QAT: 感知量化训练">​</a></h3>
<p>在训练感知量化（QAT, <strong>Quantization-Aware Training</strong>）中，模型一般在预训练过程中被量化，然后又在训练数据上再次微调，得到最后的量化模型。</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ptq-后训练量化">PTQ: 后训练量化<a href="#ptq-后训练量化" class="hash-link" aria-label="Direct link to PTQ: 后训练量化" title="Direct link to PTQ: 后训练量化">​</a></h3>
<p>后训练量化（PTQ, <strong>Post-Training Quantization</strong>）一般指在模型预训练完成后，基于校准数据集（<strong>calibration dataset</strong>）确定量化参数进而对模型进行量化。</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gptq-分组精度调优量化">GPTQ: 分组精度调优量化<a href="#gptq-分组精度调优量化" class="hash-link" aria-label="Direct link to GPTQ: 分组精度调优量化" title="Direct link to GPTQ: 分组精度调优量化">​</a></h3>
<p>GPTQ（<strong>Group-wise Precision Tuning Quantization</strong>）是一种静态的后训练量化技术。<br>
<!-- -->“静态”指的是预训练模型一旦确定，经过量化后量化参数不再更改。<br>
<strong>GPTQ 的特点</strong>：</p>
<ul>
<li>将 fp16 精度模型量化为 4-bit</li>
<li>节省约 75% 的显存</li>
<li>大幅提高推理速度</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> TechxGenus/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">GPTQ</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="awq-激活感知层量化">AWQ: 激活感知层量化<a href="#awq-激活感知层量化" class="hash-link" aria-label="Direct link to AWQ: 激活感知层量化" title="Direct link to AWQ: 激活感知层量化">​</a></h3>
<p>AWQ（<strong>Activation-Aware Layer Quantization</strong>）是一种静态的后训练量化技术。其核心思想为：少部分重要权重保持不被量化，以维持模型性能。<br>
<strong>AWQ 的优势</strong>：</p>
<ul>
<li>所需校准数据集更小</li>
<li>在指令微调和多模态模型上表现良好</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> TechxGenus/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">AWQ</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="aqlm-语言模型加法量化">AQLM: 语言模型加法量化<a href="#aqlm-语言模型加法量化" class="hash-link" aria-label="Direct link to AQLM: 语言模型加法量化" title="Direct link to AQLM: 语言模型加法量化">​</a></h3>
<p>AQLM（<strong>Additive Quantization of Language Models</strong>）是一种仅对模型权重进行量化的 PTQ 方法。<br>
<strong>特点</strong>：</p>
<ul>
<li>在 2-bit 量化下达到了最佳性能</li>
<li>在 3-bit 和 4-bit 量化下同样表现优异</li>
<li>2-bit 量化适用于低显存部署大模型</li>
</ul>
<p>尽管 AQLM 的推理速度提升并不显著，但其极低的显存占用具有很高实用价值。</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="oftq-动态后训练量化">OFTQ: 动态后训练量化<a href="#oftq-动态后训练量化" class="hash-link" aria-label="Direct link to OFTQ: 动态后训练量化" title="Direct link to OFTQ: 动态后训练量化">​</a></h3>
<p>OFTQ（<strong>On-the-fly Quantization</strong>）无需校准数据集，直接在推理阶段进行动态量化。<br>
<strong>特点</strong>：</p>
<ul>
<li>无需校准数据集</li>
<li>能够在推理阶段动态量化，保持性能</li>
</ul>
<p><strong>使用示例</strong>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#000000;--prism-background-color:#ffffff"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#000000;background-color:#ffffff"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#000000"><span class="token key atrule">model_name_or_path</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">llama/Meta</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Llama</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">3</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">8B</span><span class="token punctuation" style="color:rgb(4, 81, 165)">-</span><span class="token plain">Instruct</span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token key atrule">quantization_bit</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(9, 134, 88)">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#000000"><span class="token plain"></span><span class="token key atrule">quantization_method</span><span class="token punctuation" style="color:rgb(4, 81, 165)">:</span><span class="token plain"> bitsandbytes  </span><span class="token comment" style="color:rgb(0, 128, 0)"># 可选: [bitsandbytes (4/8), hqq (2/3/4/5/6/8), eetq (8)]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bitsandbytes-动态后训练量化">bitsandbytes: 动态后训练量化<a href="#bitsandbytes-动态后训练量化" class="hash-link" aria-label="Direct link to bitsandbytes: 动态后训练量化" title="Direct link to bitsandbytes: 动态后训练量化">​</a></h3>
<p>区别于 GPTQ，<strong>bitsandbytes</strong> 是一种动态的后训练量化技术。<br>
<strong>特点</strong>：</p>
<ul>
<li>支持大于 1B 的模型量化</li>
<li>在 8-bit 量化后，性能损失极小</li>
<li>能够节省约 50% 的显存</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hqq-半二次量化">HQQ: 半二次量化<a href="#hqq-半二次量化" class="hash-link" aria-label="Direct link to HQQ: 半二次量化" title="Direct link to HQQ: 半二次量化">​</a></h3>
<p>HQQ（<strong>Half-Quadratic Quantization</strong>）在准确度和速度之间取得平衡。<br>
<strong>特点</strong>：</p>
<ul>
<li>不需要校准阶段</li>
<li>推理速度极快</li>
<li>准确度与需要校准数据的方法相当</li>
</ul>
<p>HQQ 是一种动态的后训练量化方法，适合需要快速推理且性能敏感的场景。</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="eetq-高效变换器量化">EETQ: 高效变换器量化<a href="#eetq-高效变换器量化" class="hash-link" aria-label="Direct link to EETQ: 高效变换器量化" title="Direct link to EETQ: 高效变换器量化">​</a></h3>
<p>EETQ（<strong>Easy and Efficient Quantization for Transformers</strong>）是一种只对模型权重进行量化的 PTQ 方法。<br>
<strong>特点</strong>：</p>
<ul>
<li>速度快</li>
<li>简单易用</li>
</ul>
<p>EETQ 特别适合 对性能和实现复杂度都有较高要求的用户。</p><div class="margin-vert--lg" style="display:flex;align-items:center;justify-content:center"><a class="button button--link" href="https://github.com/jiangyangcreate/jiangyangcreate.github.io/blob/main/docs/docs/机器学习/Transformer/模型微调.md" target="_blank" rel="noreferrer noopener" aria-label="edit on github" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 512 512" height="20" width="20" style="margin-right:7px"><g transform="translate(0.000000,512.000000) scale(0.100000,-0.100000)" fill="url(#grad1)" stroke="none"><path d="M2384 5060 c-1012 -69 -1891 -738 -2235 -1703 -53 -146 -102 -349 -125 -512 -23 -157 -25 -496 -5 -655 102 -816 575 -1526 1287 -1932 164 -94 418 -200 483 -202 51 -1 94 20 116 57 18 30 19 50 16 267 -1 129 -3 236 -4 236 -1 1 -38 -4 -82 -12 -263 -43 -497 16 -646 165 -58 58 -76 86 -134 206 -94 196 -149 271 -255 348 -78 56 -120 98 -120 119 0 50 121 67 217 30 125 -47 208 -119 299 -258 126 -191 284 -276 485 -261 77 6 233 49 245 68 3 6 13 38 20 71 19 80 68 181 110 228 40 45 44 42 -86 60 -103 14 -282 60 -368 94 -362 143 -571 410 -649 829 -25 134 -25 463 0 567 36 149 103 285 195 398 l43 53 -15 47 c-52 161 -46 370 16 564 20 61 24 67 54 73 107 20 352 -68 576 -206 l98 -61 87 20 c204 47 298 57 553 57 256 0 349 -10 553 -57 l88 -20 77 48 c153 96 304 167 407 193 85 21 159 31 191 25 28 -5 33 -12 52 -72 57 -178 67 -404 23 -542 l-21 -65 20 -25 c98 -121 175 -269 212 -409 26 -98 36 -348 19 -488 -74 -608 -407 -935 -1041 -1023 -120 -16 -118 -15 -83 -53 46 -48 90 -134 115 -223 20 -74 22 -106 27 -529 7 -492 5 -482 66 -507 47 -20 102 -13 200 26 348 135 622 311 884 569 417 410 679 942 752 1527 20 159 18 498 -5 655 -173 1219 -1151 2131 -2373 2215 -161 11 -175 11 -339 0z"></path></g></svg></a><a class="button button--link" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fjiangmiemie.com%2Fdocs%2F%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%2FTransformer%2F%25E6%25A8%25A1%25E5%259E%258B%25E5%25BE%25AE%25E8%25B0%2583&amp;text=%E6%88%91%E5%88%9A%E5%88%9A%E8%AF%BB%E4%BA%86%20%22%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%22%20by%20%40jiangyangcreate" target="_blank" rel="noreferrer noopener" aria-label="share on twitter" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 24 24" height="20" width="20" style="margin-right:7px"><g fill="url(#grad1)"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></g></svg></a><a class="button button--link" href="https://jiangmiemie.com/blog/rss.xml" target="_blank" rel="noreferrer noopener" aria-label="rss reader link" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 16 16" height="20" width="20" style="margin-right:7px"><g fill="url(#grad1)"><linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#D8262C;stop-opacity:1"></stop><stop offset="100%" style="stop-color:#E6B800;stop-opacity:1"></stop></linearGradient><circle cx="3" cy="13" r="2"></circle><path d="M1 5.667v2.667A6.674 6.674 0 0 1 7.667 15h2.666c0-5.146-4.187-9.333-9.333-9.333z"></path><path d="M1 1v2.667C7.25 3.667 12.334 8.75 12.334 15H15C15 7.28 8.72 1 1 1z"></path></g></svg></a><a class="button button--link" href="mailto:jiangyangcreate@gmail.com" target="_blank" rel="noreferrer noopener" aria-label="send email" style="display:inline-flex;align-items:center"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 1024 1024" style="margin-right:7px;height:20px;width:20px"><g><path style="opacity:0.996;fill:url(#grad1)" d="M 243.5,420.5 C 189.523,379.185 135.857,337.518 82.5,295.5C 140.987,183.298 229.487,104.632 348,59.5C 432.42,30.213 518.587,23.713 606.5,40C 693.576,57.6997 769.243,96.6997 833.5,157C 787.833,202.667 742.167,248.333 696.5,294C 695.5,294.667 694.5,294.667 693.5,294C 642.378,246.775 581.711,222.942 511.5,222.5C 407.283,227.337 327.117,273.337 271,360.5C 259.938,379.624 250.771,399.624 243.5,420.5 Z"></path></g><g><path style="opacity:0.996;fill:url(#grad1)" d="M 82.5,295.5 C 135.857,337.518 189.523,379.185 243.5,420.5C 223.5,481.168 223.5,541.835 243.5,602.5C 189.523,643.815 135.857,685.482 82.5,727.5C 31.7513,624.612 18.5846,516.945 43,404.5C 51.5465,366.35 64.7132,330.016 82.5,295.5 Z"></path></g><g><path style="opacity:0.998;fill:url(#grad1)" d="M 829.5,876.5 C 777.833,835.833 726.167,795.167 674.5,754.5C 725.318,718.61 756.985,669.944 769.5,608.5C 683.503,607.5 597.503,607.167 511.5,607.5C 511.5,546.167 511.5,484.833 511.5,423.5C 662.5,423.5 813.5,423.5 964.5,423.5C 979.017,509.532 974.517,594.532 951,678.5C 928.206,755.703 887.706,821.703 829.5,876.5 Z"></path></g><g><path style="opacity:0.997;fill:url(#grad1)" d="M 243.5,602.5 C 274.71,690.744 334.043,751.577 421.5,785C 487.78,805.861 553.78,804.861 619.5,782C 638.723,774.388 657.056,765.222 674.5,754.5C 726.167,795.167 777.833,835.833 829.5,876.5C 762.129,936.339 683.462,972.839 593.5,986C 437.233,1008.45 300.566,967.451 183.5,863C 141.888,823.583 108.222,778.416 82.5,727.5C 135.857,685.482 189.523,643.815 243.5,602.5 Z"></path></g></svg></a></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/机器学习/Transformer/分布式训练/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">分布式训练</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/大模型的应用/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">大模型的应用</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#微调" class="table-of-contents__link toc-highlight">微调</a><ul><li><a href="#lora-微调" class="table-of-contents__link toc-highlight">LoRA 微调</a></li><li><a href="#冻结层微调freeze" class="table-of-contents__link toc-highlight">冻结层微调（Freeze）</a></li><li><a href="#全量微调full-fine-tuning" class="table-of-contents__link toc-highlight">全量微调（Full Fine-tuning）</a></li></ul></li><li><a href="#量化" class="table-of-contents__link toc-highlight">量化</a><ul><li><a href="#qat-感知量化训练" class="table-of-contents__link toc-highlight">QAT: 感知量化训练</a></li><li><a href="#ptq-后训练量化" class="table-of-contents__link toc-highlight">PTQ: 后训练量化</a></li><li><a href="#gptq-分组精度调优量化" class="table-of-contents__link toc-highlight">GPTQ: 分组精度调优量化</a></li><li><a href="#awq-激活感知层量化" class="table-of-contents__link toc-highlight">AWQ: 激活感知层量化</a></li><li><a href="#aqlm-语言模型加法量化" class="table-of-contents__link toc-highlight">AQLM: 语言模型加法量化</a></li><li><a href="#oftq-动态后训练量化" class="table-of-contents__link toc-highlight">OFTQ: 动态后训练量化</a></li><li><a href="#bitsandbytes-动态后训练量化" class="table-of-contents__link toc-highlight">bitsandbytes: 动态后训练量化</a></li><li><a href="#hqq-半二次量化" class="table-of-contents__link toc-highlight">HQQ: 半二次量化</a></li><li><a href="#eetq-高效变换器量化" class="table-of-contents__link toc-highlight">EETQ: 高效变换器量化</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright"><div style="font-size: 0.75rem;">Copyright ©  jiangyang 2025<div></div></div></div></div></footer></div>
</body>
</html>